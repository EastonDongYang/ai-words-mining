#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from advanced_web_scraper import AdvancedWebScraper
import json

def test_advanced_scraper():
    """æµ‹è¯•æ”¹è¿›çš„çˆ¬è™«åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„AIå·¥å…·çˆ¬è™«...")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = AdvancedWebScraper()
    
    # æµ‹è¯•URL
    test_url = "https://theresanaiforthat.com/trending/week/top-50/?pos=1"
    
    try:
        # çˆ¬å–æ•°æ®
        print(f"ğŸ¯ ç›®æ ‡URL: {test_url}")
        tools = scraper.scrape_ai_tools(test_url)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸè·å– {len(tools)} ä¸ªAIå·¥å…·")
        
        if tools:
            print("\nğŸ† å‰5ä¸ªå·¥å…·é¢„è§ˆ:")
            for i, tool in enumerate(tools[:5]):
                print(f"\n{i+1}. {tool.get('name', 'Unknown')}")
                print(f"   æè¿°: {tool.get('description', 'No description')[:100]}...")
                print(f"   åˆ†ç±»: {tool.get('category', 'Unknown')}")
                print(f"   é“¾æ¥: {tool.get('link', 'No link')}")
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            output_file = "scraped_tools_advanced.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(tools, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # åˆ†æç»“æœ
            print("\nğŸ“ˆ æ•°æ®åˆ†æ:")
            categories = {}
            for tool in tools:
                cat = tool.get('category', 'Unknown')
                categories[cat] = categories.get(cat, 0) + 1
            
            print(f"ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"   {cat}: {count} ä¸ªå·¥å…·")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_metrics = {
                'has_name': sum(1 for tool in tools if tool.get('name') and len(tool.get('name', '').strip()) > 2),
                'has_description': sum(1 for tool in tools if tool.get('description') and len(tool.get('description', '').strip()) > 10),
                'has_link': sum(1 for tool in tools if tool.get('link') and 'http' in tool.get('link', '')),
                'has_category': sum(1 for tool in tools if tool.get('category') and tool.get('category') != 'Unknown')
            }
            
            print(f"\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
            print(f"   æœ‰æ•ˆåç§°: {quality_metrics['has_name']}/{len(tools)} ({quality_metrics['has_name']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆæè¿°: {quality_metrics['has_description']}/{len(tools)} ({quality_metrics['has_description']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆé“¾æ¥: {quality_metrics['has_link']}/{len(tools)} ({quality_metrics['has_link']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆåˆ†ç±»: {quality_metrics['has_category']}/{len(tools)} ({quality_metrics['has_category']/len(tools)*100:.1f}%)")
            
            return True
            
        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•å·¥å…·æ•°æ®")
            print("ğŸ’¡ å¯èƒ½åŸå› :")
            print("   - ç½‘ç«™ç»“æ„å‘ç”Ÿå˜åŒ–")
            print("   - ç½‘ç«™æœ‰åçˆ¬è™«ä¿æŠ¤")
            print("   - ç½‘ç»œè¿æ¥é—®é¢˜")
            print("   - éœ€è¦è°ƒæ•´é€‰æ‹©å™¨ç­–ç•¥")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = test_advanced_scraper()
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æ”¹è¿›çš„çˆ¬è™«æµ‹è¯•æˆåŠŸï¼")
    else:
        print("âŒ æ”¹è¿›çš„çˆ¬è™«æµ‹è¯•å¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–å®‰è£…")
    print("=" * 60) 