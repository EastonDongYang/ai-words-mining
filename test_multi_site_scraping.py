#!/usr/bin/env python3
"""
æµ‹è¯•å¤šç½‘ç«™çˆ¬å–åŠŸèƒ½
"""

import sys
import os
from datetime import datetime

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from config import Config
from src.multi_site_scraper import MultiSiteScraper

def test_individual_sites():
    """æµ‹è¯•å•ä¸ªç½‘ç«™çš„çˆ¬å–æ•ˆæœ"""
    print("ğŸ§ª æµ‹è¯•å•ä¸ªç½‘ç«™çˆ¬å–åŠŸèƒ½...")
    
    config = Config()
    scraper = MultiSiteScraper()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - å¤šç½‘ç«™çˆ¬å–: {config.ENABLE_MULTI_SITE}")
    print(f"  - æ€»æ•°é™åˆ¶: {config.MAX_TOTAL_ITEMS}")
    print(f"  - ç›®æ ‡ç½‘ç«™æ•°: {len(config.TARGET_URLS)}")
    print(f"  - å¯ç”¨çš„ç½‘ç«™: {len(config.get_enabled_sites())}")
    
    results = {}
    
    for url in config.TARGET_URLS:
        try:
            from urllib.parse import urlparse
            site_domain = urlparse(url).netloc
            site_config = config.get_site_config(site_domain)
            
            print(f"\nğŸ•·ï¸ æµ‹è¯• {site_domain}...")
            print(f"  - URL: {url}")
            print(f"  - å¯ç”¨çŠ¶æ€: {'âœ…' if site_config.get('enabled', True) else 'âŒ'}")
            print(f"  - æœ€å¤§é¡¹ç›®æ•°: {site_config.get('max_items', 30)}")
            print(f"  - ä½¿ç”¨Selenium: {'âœ…' if site_config.get('use_selenium', True) else 'âŒ'}")
            
            if not site_config.get('enabled', True):
                print(f"  âš ï¸ ç½‘ç«™å·²ç¦ç”¨ï¼Œè·³è¿‡")
                results[site_domain] = {'status': 'disabled', 'count': 0}
                continue
            
            # å°è¯•çˆ¬å–
            start_time = datetime.now()
            tools = scraper.scrape_site(url, site_config)
            end_time = datetime.now()
            
            execution_time = (end_time - start_time).total_seconds()
            
            if tools:
                print(f"  âœ… æˆåŠŸçˆ¬å– {len(tools)} ä¸ªé¡¹ç›® (è€—æ—¶: {execution_time:.2f}ç§’)")
                results[site_domain] = {
                    'status': 'success', 
                    'count': len(tools),
                    'execution_time': execution_time,
                    'sample_items': tools[:3]  # ä¿å­˜å‰3ä¸ªæ ·æœ¬
                }
            else:
                print(f"  âŒ æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›® (è€—æ—¶: {execution_time:.2f}ç§’)")
                results[site_domain] = {
                    'status': 'no_data', 
                    'count': 0,
                    'execution_time': execution_time
                }
                
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
            results[site_domain] = {'status': 'error', 'count': 0, 'error': str(e)}
    
    return results

def test_multi_site_scraping():
    """æµ‹è¯•å¤šç½‘ç«™ç»¼åˆçˆ¬å–"""
    print("\nğŸŒ æµ‹è¯•å¤šç½‘ç«™ç»¼åˆçˆ¬å–...")
    
    scraper = MultiSiteScraper()
    
    try:
        start_time = datetime.now()
        all_tools = scraper.scrape_all_sites()
        end_time = datetime.now()
        
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nğŸ“Š ç»¼åˆçˆ¬å–ç»“æœ:")
        print(f"  - æ€»é¡¹ç›®æ•°: {len(all_tools)}")
        print(f"  - æ€»è€—æ—¶: {execution_time:.2f}ç§’")
        print(f"  - å¹³å‡æ¯é¡¹è€—æ—¶: {execution_time/len(all_tools):.2f}ç§’" if all_tools else "  - æ— æ•°æ®")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_stats = {}
        for tool in all_tools:
            source = tool.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print(f"\nğŸ“ˆ æŒ‰æ¥æºç»Ÿè®¡:")
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {source}: {count} ä¸ªé¡¹ç›®")
        
        return all_tools
        
    except Exception as e:
        print(f"âŒ å¤šç½‘ç«™çˆ¬å–å¤±è´¥: {e}")
        return []

def analyze_results(individual_results, multi_site_results):
    """åˆ†æçˆ¬å–ç»“æœ"""
    print("\nğŸ“‹ ç»“æœåˆ†æ:")
    
    # ç»Ÿè®¡å„ç½‘ç«™çŠ¶æ€
    enabled_count = sum(1 for r in individual_results.values() if r['status'] != 'disabled')
    success_count = sum(1 for r in individual_results.values() if r['status'] == 'success')
    failed_count = sum(1 for r in individual_results.values() if r['status'] in ['error', 'no_data'])
    
    print(f"  - å¯ç”¨çš„ç½‘ç«™: {enabled_count}")
    print(f"  - æˆåŠŸçˆ¬å–: {success_count}")
    print(f"  - å¤±è´¥/æ— æ•°æ®: {failed_count}")
    print(f"  - ç»¼åˆç»“æœ: {len(multi_site_results)} ä¸ªé¡¹ç›®")
    
    # è¯†åˆ«é—®é¢˜ç½‘ç«™
    problem_sites = []
    for site, result in individual_results.items():
        if result['status'] in ['error', 'no_data']:
            problem_sites.append(site)
    
    if problem_sites:
        print(f"\nâš ï¸ éœ€è¦å…³æ³¨çš„ç½‘ç«™:")
        for site in problem_sites:
            result = individual_results[site]
            print(f"  - {site}: {result['status']}")
            if 'error' in result:
                print(f"    é”™è¯¯ä¿¡æ¯: {result['error']}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šç½‘ç«™çˆ¬å–æµ‹è¯•...")
    print("="*60)
    
    try:
        # æµ‹è¯•å•ä¸ªç½‘ç«™
        individual_results = test_individual_sites()
        
        # æµ‹è¯•ç»¼åˆçˆ¬å–
        multi_site_results = test_multi_site_scraping()
        
        # åˆ†æç»“æœ
        analyze_results(individual_results, multi_site_results)
        
        print("\nâœ… æµ‹è¯•å®Œæˆ!")
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥å¤±è´¥çš„ç½‘ç«™ï¼Œå¯èƒ½éœ€è¦æ›´æ–°çˆ¬è™«é€»è¾‘")
        print("2. å¦‚æœæ•°æ®é‡ä»ç„¶ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´max_itemsé…ç½®")
        print("3. è€ƒè™‘æ·»åŠ æ›´å¤šç›®æ ‡ç½‘ç«™")
        print("4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒChrome WebDriverè®¾ç½®")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

if __name__ == "__main__":
    main() 