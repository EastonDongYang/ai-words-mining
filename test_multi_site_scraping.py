#!/usr/bin/env python3
"""
æµ‹è¯•å¤šç½‘ç«™çˆ¬è™«åŠŸèƒ½
Test Multi-Site Scraping Functionality
"""

import sys
import os
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from config import Config
from src.multi_site_scraper import MultiSiteScraper

def test_multi_site_scraping():
    """æµ‹è¯•å¤šç½‘ç«™çˆ¬è™«åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤šç½‘ç«™çˆ¬è™«åŠŸèƒ½...")
    print("=" * 60)
    
    try:
        # åˆ›å»ºé…ç½®å’Œçˆ¬è™«å®ä¾‹
        config = Config()
        scraper = MultiSiteScraper()
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print("ğŸ“‹ é…ç½®ä¿¡æ¯ï¼š")
        print(f"  å¤šç½‘ç«™çˆ¬è™«å¯ç”¨: {config.ENABLE_MULTI_SITE}")
        print(f"  ç›®æ ‡ç½‘ç«™æ•°é‡: {len(config.TARGET_URLS)}")
        print(f"  æœ€å¤§æ€»æ•°é‡: {config.MAX_TOTAL_ITEMS}")
        print(f"  è°ƒè¯•æ¨¡å¼: {config.DEBUG_MODE}")
        print()
        
        # æ‰“å°è¦çˆ¬å–çš„ç½‘ç«™
        print("ğŸŒ ç›®æ ‡ç½‘ç«™åˆ—è¡¨ï¼š")
        for i, url in enumerate(config.TARGET_URLS, 1):
            print(f"  {i}. {url}")
        print()
        
        # æ‰“å°å¯ç”¨çš„ç½‘ç«™é…ç½®
        print("âš™ï¸ ç½‘ç«™é…ç½®ï¼š")
        for site in config.get_enabled_sites():
            site_config = config.get_site_config(site)
            print(f"  {site}:")
            print(f"    - å¯ç”¨: {site_config.get('enabled', True)}")
            print(f"    - æœ€å¤§æ•°é‡: {site_config.get('max_items', 30)}")
            print(f"    - å»¶è¿Ÿ: {site_config.get('delay', 2)}ç§’")
            print(f"    - ä½¿ç”¨Selenium: {site_config.get('use_selenium', True)}")
        print()
        
        # å¼€å§‹çˆ¬å–æµ‹è¯•
        print("ğŸš€ å¼€å§‹çˆ¬å–æµ‹è¯•...")
        start_time = datetime.now()
        
        # æ‰§è¡Œçˆ¬å–
        all_tools = scraper.scrape_all_sites()
        
        end_time = datetime.now()
        execution_time = end_time - start_time
        
        # åˆ†æç»“æœ
        print(f"â±ï¸ çˆ¬å–è€—æ—¶: {execution_time}")
        print(f"ğŸ“Š æ€»å…±è·å–: {len(all_tools)} ä¸ªå·¥å…·/äº§å“")
        
        if all_tools:
            # æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡
            source_stats = {}
            for tool in all_tools:
                source = tool.get('source', 'unknown')
                source_stats[source] = source_stats.get(source, 0) + 1
            
            print("\nğŸ“ˆ æ¥æºç»Ÿè®¡:")
            for source, count in sorted(source_stats.items()):
                print(f"  - {source}: {count} ä¸ª")
            
            # æ˜¾ç¤ºå‰10ä¸ªå·¥å…·ç¤ºä¾‹
            print("\nğŸ” å‰10ä¸ªå·¥å…·ç¤ºä¾‹:")
            for i, tool in enumerate(all_tools[:10], 1):
                print(f"  {i}. {tool.get('name', 'Unknown')}")
                print(f"     æ¥æº: {tool.get('source', 'unknown')}")
                print(f"     æè¿°: {tool.get('description', 'No description')[:100]}...")
                print(f"     ç±»åˆ«: {', '.join(tool.get('categories', []))}")
                print()
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"test_multi_site_results_{timestamp}.json"
            scraper.save_results(all_tools, filename)
            
            print(f"âœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            return True
        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_single_site():
    """æµ‹è¯•å•ä¸ªç½‘ç«™çˆ¬å–"""
    print("\nğŸ§ª æµ‹è¯•å•ä¸ªç½‘ç«™çˆ¬å–...")
    
    try:
        config = Config()
        scraper = MultiSiteScraper()
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªç½‘ç«™
        test_url = config.TARGET_URLS[0]
        print(f"ğŸ”— æµ‹è¯•ç½‘ç«™: {test_url}")
        
        from urllib.parse import urlparse
        site_domain = urlparse(test_url).netloc
        site_config = config.get_site_config(site_domain)
        
        tools = scraper.scrape_site(test_url, site_config)
        
        if tools:
            print(f"âœ… æˆåŠŸè·å– {len(tools)} ä¸ªå·¥å…·")
            print("ğŸ“ ç¤ºä¾‹å·¥å…·:")
            for i, tool in enumerate(tools[:3], 1):
                print(f"  {i}. {tool.get('name', 'Unknown')}")
                print(f"     æè¿°: {tool.get('description', 'No description')[:80]}...")
        else:
            print("âš ï¸ æ²¡æœ‰è·å–åˆ°å·¥å…·")
            
    except Exception as e:
        print(f"âŒ å•ç½‘ç«™æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¤šç½‘ç«™çˆ¬è™«æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # æµ‹è¯•å¤šç½‘ç«™çˆ¬å–
    success = test_multi_site_scraping()
    
    # æµ‹è¯•å•ç½‘ç«™çˆ¬å–
    test_single_site()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ æç¤º: æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶è·å–è¯¦ç»†ç»“æœ")
    else:
        print("âš ï¸ æµ‹è¯•è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé…ç½®")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 