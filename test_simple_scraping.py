#!/usr/bin/env python3
"""
ç®€åŒ–çš„çˆ¬è™«æµ‹è¯• - ä¸ä½¿ç”¨Selenium
Simple Scraping Test - Without Selenium
"""

import sys
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from config import Config

def test_basic_requests():
    """æµ‹è¯•åŸºæœ¬çš„HTTPè¯·æ±‚åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºæœ¬HTTPè¯·æ±‚åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•ç®€å•çš„HTTPè¯·æ±‚
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # æµ‹è¯•ä¸€ä¸ªç®€å•çš„ç½‘ç«™
        test_url = "https://httpbin.org/json"
        response = session.get(test_url, timeout=10)
        
        if response.status_code == 200:
            print("âœ… HTTPè¯·æ±‚åŠŸèƒ½æ­£å¸¸")
            return True
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ HTTPè¯·æ±‚æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_html_parsing():
    """æµ‹è¯•HTMLè§£æåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•HTMLè§£æåŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæµ‹è¯•HTML
        test_html = """
        <html>
        <head><title>Test</title></head>
        <body>
            <div class="tool-card">
                <h3 class="name">Test AI Tool</h3>
                <p class="description">This is a test AI tool description</p>
                <span class="tag">AI</span>
                <span class="tag">Test</span>
            </div>
        </body>
        </html>
        """
        
        soup = BeautifulSoup(test_html, 'html.parser')
        
        # æµ‹è¯•æå–åŠŸèƒ½
        name = soup.find('h3', class_='name').get_text(strip=True)
        description = soup.find('p', class_='description').get_text(strip=True)
        tags = [tag.get_text(strip=True) for tag in soup.find_all('span', class_='tag')]
        
        if name == "Test AI Tool" and "test AI tool" in description.lower() and len(tags) == 2:
            print("âœ… HTMLè§£æåŠŸèƒ½æ­£å¸¸")
            return True
        else:
            print("âŒ HTMLè§£æåŠŸèƒ½å¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ HTMLè§£ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_betalist_scraping():
    """æµ‹è¯•BetaListç½‘ç«™çˆ¬å–ï¼ˆä½¿ç”¨requestsï¼‰"""
    print("ğŸ§ª æµ‹è¯•BetaListç½‘ç«™çˆ¬å–...")
    
    try:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
        })
        
        url = "https://betalist.com/"
        print(f"æ­£åœ¨è®¿é—®: {url}")
        
        response = session.get(url, timeout=30)
        print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å¯èƒ½çš„äº§å“å…ƒç´ 
            possible_selectors = [
                'div[class*="startup"]',
                'div[class*="product"]', 
                'article',
                'div[class*="item"]',
                'div[class*="card"]'
            ]
            
            found_elements = []
            for selector in possible_selectors:
                elements = soup.select(selector)
                if elements:
                    found_elements.extend(elements)
                    print(f"æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
            
            if found_elements:
                print(f"âœ… æ€»å…±æ‰¾åˆ° {len(found_elements)} ä¸ªå¯èƒ½çš„äº§å“å…ƒç´ ")
                
                # å°è¯•æå–å‰å‡ ä¸ªå…ƒç´ çš„ä¿¡æ¯
                extracted = 0
                for element in found_elements[:10]:
                    # å°è¯•æå–æ ‡é¢˜
                    title_tags = element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                    for title_tag in title_tags:
                        title_text = title_tag.get_text(strip=True)
                        if title_text and len(title_text) > 3 and len(title_text) < 100:
                            print(f"  - å‘ç°äº§å“: {title_text}")
                            extracted += 1
                            break
                
                if extracted > 0:
                    print(f"âœ… æˆåŠŸæå– {extracted} ä¸ªäº§å“ä¿¡æ¯")
                    return True
                else:
                    print("âš ï¸ æ‰¾åˆ°äº†å…ƒç´ ä½†æœªèƒ½æå–äº§å“ä¿¡æ¯")
                    return False
            else:
                print("âš ï¸ æœªæ‰¾åˆ°äº§å“å…ƒç´ ")
                return False
        else:
            print(f"âŒ ç½‘ç«™è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ BetaListçˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        config = Config()
        
        print(f"ç›®æ ‡ç½‘ç«™æ•°é‡: {len(config.TARGET_URLS)}")
        print(f"å¤šç½‘ç«™çˆ¬è™«å¯ç”¨: {config.ENABLE_MULTI_SITE}")
        print(f"æœ€å¤§æ€»æ•°é‡: {config.MAX_TOTAL_ITEMS}")
        
        if len(config.TARGET_URLS) >= 6 and config.ENABLE_MULTI_SITE:
            print("âœ… é…ç½®åŠ è½½æ­£å¸¸")
            return True
        else:
            print("âŒ é…ç½®åŠ è½½å¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_openai_analyzer():
    """æµ‹è¯•OpenAIåˆ†æå™¨ï¼ˆä¸å®é™…è°ƒç”¨APIï¼‰"""
    print("ğŸ§ª æµ‹è¯•OpenAIåˆ†æå™¨é…ç½®...")
    
    try:
        # åªæµ‹è¯•å¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œä¸å®é™…è°ƒç”¨API
        from src.openai_analyzer import OpenAIAnalyzer
        
        analyzer = OpenAIAnalyzer()
        
        if hasattr(analyzer, 'config') and hasattr(analyzer, 'client'):
            print("âœ… OpenAIåˆ†æå™¨é…ç½®æ­£å¸¸")
            return True
        else:
            print("âŒ OpenAIåˆ†æå™¨é…ç½®å¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ OpenAIåˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ ç®€åŒ–çˆ¬è™«åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("é…ç½®åŠ è½½", test_config_loading),
        ("åŸºæœ¬HTTPè¯·æ±‚", test_basic_requests),
        ("HTMLè§£æåŠŸèƒ½", test_html_parsing),
        ("OpenAIåˆ†æå™¨", test_openai_analyzer),
        ("BetaListçˆ¬å–", test_betalist_scraping),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name}:")
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ’¡ Chrome WebDriveré—®é¢˜éœ€è¦å•ç‹¬è§£å†³")
    elif passed >= total // 2:
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸ï¼Œéœ€è¦è§£å†³ä¸€äº›é—®é¢˜")
    else:
        print("âŒ å¤šæ•°åŠŸèƒ½å¼‚å¸¸ï¼Œéœ€è¦æ£€æŸ¥ç¯å¢ƒé…ç½®")
    
    return passed >= total // 2

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 