#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import requests
import json
import time
import random
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin

class GoogleCacheScraper:
    """
    ä¸“é—¨ä½¿ç”¨Googleç¼“å­˜çš„çˆ¬è™«
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(headers)
    
    def get_google_cache(self, url):
        """è·å–Googleç¼“å­˜å†…å®¹"""
        cache_url = f"https://webcache.googleusercontent.com/search?q=cache:{url}"
        
        try:
            print(f"ğŸ”„ è·å–Googleç¼“å­˜: {url}")
            response = self.session.get(cache_url, timeout=30)
            
            if response.status_code == 200:
                print("âœ… Googleç¼“å­˜è·å–æˆåŠŸ")
                return response.text
            else:
                print(f"âŒ Googleç¼“å­˜å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è·å–Googleç¼“å­˜æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_tools_from_cache(self, html_content, original_url):
        """ä»ç¼“å­˜çš„HTMLä¸­æå–å·¥å…·æ•°æ®"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            tools = []
            
            print("ğŸ” å¼€å§‹ä»ç¼“å­˜å†…å®¹ä¸­æå–å·¥å…·...")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å·¥å…·é“¾æ¥
            all_links = soup.find_all('a', href=True)
            print(f"ğŸ“‹ æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            for i, link in enumerate(all_links):
                try:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # è¿‡æ»¤å¯èƒ½çš„å·¥å…·é“¾æ¥
                    if (text and len(text) > 3 and len(text) < 200 and
                        not any(skip in href.lower() for skip in ['#', 'javascript:', 'mailto:', 'tel:', 'cache:', 'webcache']) and
                        not any(skip in text.lower() for skip in ['click here', 'read more', 'learn more', 'sign up', 'login', 'register'])):
                        
                        # è·å–çˆ¶å…ƒç´ çš„æ›´å¤šä¿¡æ¯
                        parent = link.parent
                        description = ""
                        
                        # å°è¯•è·å–æè¿°
                        if parent:
                            # æŸ¥æ‰¾ç›¸é‚»çš„æè¿°æ–‡æœ¬
                            for sibling in parent.find_next_siblings():
                                if sibling.get_text().strip():
                                    description = sibling.get_text().strip()[:500]
                                    break
                            
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•çˆ¶å…ƒç´ çš„æ–‡æœ¬
                            if not description:
                                parent_text = parent.get_text().strip()
                                if parent_text and len(parent_text) > len(text):
                                    description = parent_text[:500]
                        
                        # å°è¯•ä»URLæ¨æ–­åˆ†ç±»
                        category = "Unknown"
                        if any(cat in href.lower() for cat in ['ai', 'artificial-intelligence']):
                            category = "AI"
                        elif any(cat in href.lower() for cat in ['tool', 'productivity']):
                            category = "Productivity"
                        elif any(cat in href.lower() for cat in ['design', 'creative']):
                            category = "Design"
                        elif any(cat in href.lower() for cat in ['business', 'marketing']):
                            category = "Business"
                        
                        tool_data = {
                            'name': text,
                            'description': description,
                            'link': href if href.startswith('http') else urljoin(original_url, href),
                            'category': category,
                            'source': 'theresanaiforthat.com (cached)',
                            'scraped_at': datetime.now().isoformat(),
                            'index': i + 1
                        }
                        
                        tools.append(tool_data)
                        
                        if len(tools) >= 50:  # é™åˆ¶æœ€å¤š50ä¸ªå·¥å…·
                            break
                        
                except Exception as e:
                    continue
            
            # å»é‡
            unique_tools = []
            seen_names = set()
            for tool in tools:
                name = tool['name'].lower()
                if name not in seen_names and len(name) > 2:
                    seen_names.add(name)
                    unique_tools.append(tool)
            
            print(f"ğŸ“Š æå–äº† {len(unique_tools)} ä¸ªå»é‡åçš„å·¥å…·")
            return unique_tools
            
        except Exception as e:
            print(f"âŒ ä»ç¼“å­˜æå–å·¥å…·å¤±è´¥: {e}")
            return []
    
    def scrape_from_cache(self, url):
        """ä»ç¼“å­˜çˆ¬å–æ•°æ®"""
        # è·å–ç¼“å­˜å†…å®¹
        cached_html = self.get_google_cache(url)
        
        if cached_html:
            # æå–å·¥å…·æ•°æ®
            tools = self.extract_tools_from_cache(cached_html, url)
            return tools
        else:
            return []

def test_google_cache_scraper():
    """æµ‹è¯•Googleç¼“å­˜çˆ¬è™«"""
    print("ğŸš€ æµ‹è¯•Googleç¼“å­˜çˆ¬è™«...")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = GoogleCacheScraper()
    
    # æµ‹è¯•URL
    test_url = "https://theresanaiforthat.com/trending/week/top-50/?pos=1"
    
    try:
        # çˆ¬å–æ•°æ®
        print(f"ğŸ¯ ç›®æ ‡URL: {test_url}")
        tools = scraper.scrape_from_cache(test_url)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸè·å– {len(tools)} ä¸ªAIå·¥å…·")
        
        if tools:
            print("\nğŸ† å‰10ä¸ªå·¥å…·é¢„è§ˆ:")
            for i, tool in enumerate(tools[:10]):
                print(f"\n{i+1}. {tool.get('name', 'Unknown')}")
                print(f"   æè¿°: {tool.get('description', 'No description')[:100]}...")
                print(f"   åˆ†ç±»: {tool.get('category', 'Unknown')}")
                print(f"   é“¾æ¥: {tool.get('link', 'No link')}")
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            output_file = "scraped_tools_cache.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(tools, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # åˆ†æç»“æœ
            print("\nğŸ“ˆ æ•°æ®åˆ†æ:")
            categories = {}
            for tool in tools:
                cat = tool.get('category', 'Unknown')
                categories[cat] = categories.get(cat, 0) + 1
            
            print(f"ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"   {cat}: {count} ä¸ªå·¥å…·")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_metrics = {
                'has_name': sum(1 for tool in tools if tool.get('name') and len(tool.get('name', '').strip()) > 2),
                'has_description': sum(1 for tool in tools if tool.get('description') and len(tool.get('description', '').strip()) > 10),
                'has_link': sum(1 for tool in tools if tool.get('link') and 'http' in tool.get('link', '')),
                'has_category': sum(1 for tool in tools if tool.get('category') and tool.get('category') != 'Unknown')
            }
            
            print(f"\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
            print(f"   æœ‰æ•ˆåç§°: {quality_metrics['has_name']}/{len(tools)} ({quality_metrics['has_name']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆæè¿°: {quality_metrics['has_description']}/{len(tools)} ({quality_metrics['has_description']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆé“¾æ¥: {quality_metrics['has_link']}/{len(tools)} ({quality_metrics['has_link']/len(tools)*100:.1f}%)")
            print(f"   æœ‰æ•ˆåˆ†ç±»: {quality_metrics['has_category']}/{len(tools)} ({quality_metrics['has_category']/len(tools)*100:.1f}%)")
            
            return True
            
        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•å·¥å…·æ•°æ®")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_google_cache_scraper()
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ Googleç¼“å­˜çˆ¬è™«æµ‹è¯•æˆåŠŸï¼")
        print("ğŸ’¡ å¯ä»¥ä½œä¸ºä¸»è¦çˆ¬è™«çš„å¤‡é€‰æ–¹æ¡ˆ")
    else:
        print("âŒ Googleç¼“å­˜çˆ¬è™«æµ‹è¯•å¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç¼“å­˜å¯ç”¨æ€§")
    print("=" * 60) 