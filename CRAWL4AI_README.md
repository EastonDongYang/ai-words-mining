# ğŸš€ Crawl4AI å‡çº§æŒ‡å—

## ğŸ“‹ ç›®å½•
- [ç®€ä»‹](#ç®€ä»‹)
- [ä¸ºä»€ä¹ˆé€‰æ‹©Crawl4AI](#ä¸ºä»€ä¹ˆé€‰æ‹©crawl4ai)
- [æ–°ç‰¹æ€§å¯¹æ¯”](#æ–°ç‰¹æ€§å¯¹æ¯”)
- [å®‰è£…å’Œè®¾ç½®](#å®‰è£…å’Œè®¾ç½®)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è¿ç§»æŒ‡å—](#è¿ç§»æŒ‡å—)

## ç®€ä»‹

åŸºäºæ‚¨æåˆ°çš„ [Crawl4AI](https://github.com/unclecode/crawl4ai) é¡¹ç›®ï¼Œæˆ‘ä¸ºæ‚¨çš„AIå·¥å…·çˆ¬è™«ç³»ç»Ÿåˆ›å»ºäº†ä¸€ä¸ªå…¨æ–°çš„ã€é«˜æ€§èƒ½çš„å®ç°ã€‚Crawl4AIæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºLLMå‹å¥½çš„å¼€æºç½‘ç»œçˆ¬è™«ï¼Œæ‹¥æœ‰47.9k GitHubæ˜Ÿæ ‡ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- ğŸš€ **6å€æ€§èƒ½æå‡** - å¼‚æ­¥å¤„ç†å’Œä¼˜åŒ–çš„èµ„æºç®¡ç†
- ğŸ§  **AIä¼˜åŒ–** - ä¸“é—¨ä¸ºLLMå¤„ç†ä¼˜åŒ–çš„Markdownè¾“å‡º
- ğŸ”§ **æ™ºèƒ½æå–** - CSSé€‰æ‹©å™¨ + LLMåŒé‡æå–ç­–ç•¥
- ğŸŒŠ **æ·±åº¦çˆ¬å–** - BFS/DFS/BestFirstæœç´¢ç®—æ³•
- ğŸ”„ **å†…å®¹è¿‡æ»¤** - æ™ºèƒ½å†…å®¹è¿‡æ»¤å’Œé™å™ª
- ğŸ“Š **ç»“æ„åŒ–æ•°æ®** - æ”¯æŒJSON Schemaå’ŒPydanticæ¨¡å‹

## ä¸ºä»€ä¹ˆé€‰æ‹©Crawl4AI

### ğŸ†š ä¸ä¼ ç»ŸSeleniumæ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»ŸSelenium | Crawl4AI |
|------|-------------|----------|
| å¯åŠ¨é€Ÿåº¦ | æ…¢ (3-5ç§’) | å¿« (0.5-1ç§’) |
| å†…å­˜ä½¿ç”¨ | é«˜ (200MB+) | ä½ (50MB+) |
| å¹¶å‘å¤„ç† | å—é™ | é«˜æ•ˆå¼‚æ­¥ |
| åçˆ¬æ£€æµ‹ | å®¹æ˜“è¢«æ£€æµ‹ | æ›´å¥½çš„éšè”½æ€§ |
| å†…å®¹è´¨é‡ | åŸå§‹HTML | AIä¼˜åŒ–Markdown |
| ç»´æŠ¤æˆæœ¬ | é«˜ | ä½ |

### ğŸ¯ ä¸»è¦æ”¹è¿›

1. **å¼‚æ­¥å¤„ç†** - ä½¿ç”¨asyncioå®ç°é«˜å¹¶å‘
2. **æ™ºèƒ½é€‰æ‹©å™¨** - é¢„å®šä¹‰çš„Schemaå‡å°‘è¯•é”™
3. **å†…å®¹è¿‡æ»¤** - è‡ªåŠ¨è¿‡æ»¤å™ªéŸ³å’Œå¹¿å‘Š
4. **ç¼“å­˜æœºåˆ¶** - é¿å…é‡å¤è¯·æ±‚
5. **é”™è¯¯å¤„ç†** - æ›´å¥å£®çš„é”™è¯¯æ¢å¤
6. **LLMé›†æˆ** - ç›´æ¥è¾“å‡ºLLMå‹å¥½çš„æ ¼å¼

## å®‰è£…å’Œè®¾ç½®

### ğŸ”§ è‡ªåŠ¨å®‰è£… (æ¨è)

```bash
# è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬
python install_crawl4ai.py
```

### ğŸ“¦ æ‰‹åŠ¨å®‰è£…

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. å®‰è£…Playwrightæµè§ˆå™¨
playwright install chromium

# 3. è¿è¡ŒCrawl4AIè®¾ç½®
crawl4ai-setup

# 4. éªŒè¯å®‰è£…
crawl4ai-doctor
```

### ğŸŒ Dockeréƒ¨ç½² (å¯é€‰)

```bash
# æ‹‰å–Crawl4AI Dockeré•œåƒ
docker pull unclecode/crawl4ai:0.7.0

# è¿è¡Œå®¹å™¨
docker run -d -p 11235:11235 --name crawl4ai --shm-size=2g unclecode/crawl4ai:0.7.0

# è®¿é—®API
curl -X POST http://localhost:11235/crawl -H "Content-Type: application/json" -d '{"urls": ["https://www.toolify.ai/"], "priority": 10}'
```

## é…ç½®è¯´æ˜

### ğŸ“ ç¯å¢ƒå˜é‡é…ç½®

å¤åˆ¶ `env.example` åˆ° `.env` å¹¶é…ç½®ï¼š

```env
# æ ¸å¿ƒé…ç½®
OPENAI_API_KEY=your-openai-api-key
USE_CRAWL4AI=true

# Crawl4AIé…ç½®
CRAWL4AI_HEADLESS=true
CRAWL4AI_VERBOSE=false
CRAWL4AI_CACHE_MODE=enabled
CRAWL4AI_MAX_CONCURRENT=5

# LLMæå–é…ç½®
ENABLE_LLM_EXTRACTION=false
LLM_PROVIDER=openai/gpt-4o-mini

# å†…å®¹è¿‡æ»¤é…ç½®
CONTENT_FILTER_TYPE=pruning
CONTENT_FILTER_THRESHOLD=0.48

# æ·±åº¦çˆ¬å–é…ç½®
ENABLE_DEEP_CRAWL=false
DEEP_CRAWL_STRATEGY=bfs
DEEP_CRAWL_MAX_PAGES=10
```

### âš™ï¸ é«˜çº§é…ç½®

```python
from config import Config

# è·å–ä¸åŒé…ç½®
crawl4ai_config = Config.get_crawl4ai_config()
llm_config = Config.get_llm_config()
filter_config = Config.get_content_filter_config()
deep_crawl_config = Config.get_deep_crawl_config()
```

## ä½¿ç”¨ç¤ºä¾‹

### ğŸ¯ åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from src.crawl4ai_scraper import Crawl4AIScraper

async def basic_example():
    scraper = Crawl4AIScraper(debug_mode=True)
    
    # é…ç½®è¦çˆ¬å–çš„ç«™ç‚¹
    urls_config = {
        "https://www.toolify.ai/": {
            "name": "toolify",
            "max_items": 30
        }
    }
    
    # ä½¿ç”¨CSSé€‰æ‹©å™¨ç­–ç•¥
    results = await scraper.scrape_multiple_sites(urls_config, use_llm=False)
    
    print(f"çˆ¬å–åˆ° {len(results)} ä¸ªAIå·¥å…·")
    for tool in results[:3]:
        print(f"- {tool['name']}: {tool['description'][:100]}...")

asyncio.run(basic_example())
```

### ğŸ¤– LLMæå–ç¤ºä¾‹

```python
async def llm_example():
    scraper = Crawl4AIScraper(debug_mode=True)
    
    urls_config = {
        "https://www.producthunt.com/topics/artificial-intelligence": {
            "name": "producthunt",
            "max_items": 20
        }
    }
    
    # ä½¿ç”¨LLMæå–ç­–ç•¥ï¼ˆéœ€è¦OpenAI API Keyï¼‰
    results = await scraper.scrape_multiple_sites(urls_config, use_llm=True)
    
    print("LLMæå–ç»“æœ:")
    for tool in results:
        print(f"å·¥å…·: {tool['name']}")
        print(f"æè¿°: {tool['description']}")
        print(f"åˆ†ç±»: {', '.join(tool['categories'])}")
        print(f"å®šä»·: {tool.get('pricing', 'æœªçŸ¥')}")
        print("-" * 50)

asyncio.run(llm_example())
```

### ğŸ”„ æ·±åº¦çˆ¬å–ç¤ºä¾‹

```python
async def deep_crawl_example():
    scraper = Crawl4AIScraper(debug_mode=True)
    
    # æ·±åº¦çˆ¬å–ä¸€ä¸ªç½‘ç«™
    results = await scraper.deep_crawl_site(
        "https://www.toolify.ai/",
        max_pages=15
    )
    
    print(f"æ·±åº¦çˆ¬å–å‘ç° {len(results)} ä¸ªé¡µé¢")
    for page in results:
        print(f"æ·±åº¦ {page['depth']}: {page['name']}")

asyncio.run(deep_crawl_example())
```

### ğŸ”€ å¹¶å‘çˆ¬å–ç¤ºä¾‹

```python
async def concurrent_example():
    scraper = Crawl4AIScraper(debug_mode=True)
    
    # é…ç½®å¤šä¸ªç«™ç‚¹
    urls_config = {
        "https://www.toolify.ai/": {"name": "toolify", "max_items": 30},
        "https://www.producthunt.com/topics/artificial-intelligence": {"name": "producthunt", "max_items": 25},
        "https://www.futuretools.io/": {"name": "futuretools", "max_items": 20}
    }
    
    # å¹¶å‘çˆ¬å–æ‰€æœ‰ç«™ç‚¹
    results = await scraper.scrape_multiple_sites(urls_config, use_llm=False)
    
    # æŒ‰æ¥æºåˆ†ç»„
    by_source = {}
    for tool in results:
        source = tool['source']
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(tool)
    
    for source, tools in by_source.items():
        print(f"{source}: {len(tools)} ä¸ªå·¥å…·")

asyncio.run(concurrent_example())
```

## æ€§èƒ½ä¼˜åŒ–

### ğŸš€ æå‡çˆ¬å–é€Ÿåº¦

```python
# 1. è°ƒæ•´å¹¶å‘æ•°
CRAWL4AI_MAX_CONCURRENT=10  # å¢åŠ å¹¶å‘æ•°

# 2. å¯ç”¨ç¼“å­˜
CRAWL4AI_CACHE_MODE=enabled

# 3. å‡å°‘ç­‰å¾…æ—¶é—´
CRAWL4AI_WAIT_TIMEOUT=15000  # 15ç§’è€Œé30ç§’

# 4. ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½
CRAWL4AI_ENABLE_SCREENSHOTS=false
CRAWL4AI_ENABLE_NETWORK_CAPTURE=false
```

### ğŸ’¡ å†…å­˜ä¼˜åŒ–

```python
# 1. é™åˆ¶æœ€å¤§é¡¹ç›®æ•°
MAX_TOTAL_ITEMS=300

# 2. ä½¿ç”¨æ›´ç²¾ç¡®çš„å†…å®¹è¿‡æ»¤
CONTENT_FILTER_TYPE=pruning
CONTENT_FILTER_THRESHOLD=0.6  # æ›´ä¸¥æ ¼çš„è¿‡æ»¤

# 3. æ§åˆ¶æ·±åº¦çˆ¬å–
DEEP_CRAWL_MAX_PAGES=5
DEEP_CRAWL_MAX_DEPTH=2
```

### ğŸ”§ è´¨é‡ä¼˜åŒ–

```python
# 1. å¯ç”¨LLMæå–ï¼ˆæ›´é«˜è´¨é‡ï¼‰
ENABLE_LLM_EXTRACTION=true
LLM_PROVIDER=openai/gpt-4o-mini

# 2. ä½¿ç”¨BM25å†…å®¹è¿‡æ»¤
CONTENT_FILTER_TYPE=bm25
BM25_QUERY=AI tools artificial intelligence machine learning

# 3. è°ƒæ•´ç­‰å¾…æ—¶é—´
CRAWL4AI_WAIT_TIMEOUT=30000  # ç»™åŠ¨æ€å†…å®¹æ›´å¤šæ—¶é—´
```

## å¸¸è§é—®é¢˜

### â“ å®‰è£…é—®é¢˜

**Q: playwright install å¤±è´¥**
```bash
# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨python -m
python -m playwright install chromium

# è§£å†³æ–¹æ¡ˆ2: æ‰‹åŠ¨å®‰è£…ä¾èµ–
sudo apt-get update
sudo apt-get install -y libgconf-2-4 libxss1 libxtst6 libxrandr2 libasound2-dev libpangocairo-1.0-0 libatk1.0-0 libcairo-gobject2 libgtk-3-0 libgdk-pixbuf2.0-0

# è§£å†³æ–¹æ¡ˆ3: ä½¿ç”¨Docker
docker run -d -p 11235:11235 --name crawl4ai --shm-size=2g unclecode/crawl4ai:0.7.0
```

**Q: crawl4ai-setup å¤±è´¥**
```bash
# é€šå¸¸å¯ä»¥å¿½ç•¥ï¼Œç›´æ¥æµ‹è¯•åŠŸèƒ½
python test_crawl4ai.py
```

### âš ï¸ è¿è¡Œæ—¶é—®é¢˜

**Q: æå–ä¸åˆ°æ•°æ®**
```python
# 1. å¯ç”¨è°ƒè¯•æ¨¡å¼
scraper = Crawl4AIScraper(debug_mode=True)

# 2. æ£€æŸ¥é€‰æ‹©å™¨
# æŸ¥çœ‹schemaæ˜¯å¦æ­£ç¡®åŒ¹é…ç½‘ç«™ç»“æ„

# 3. å¢åŠ ç­‰å¾…æ—¶é—´
wait_for_timeout=5000

# 4. å°è¯•LLMæå–
use_llm=True
```

**Q: é€Ÿåº¦å¤ªæ…¢**
```python
# 1. å‡å°‘å¹¶å‘æ•°
CRAWL4AI_MAX_CONCURRENT=3

# 2. å¯ç”¨ç¼“å­˜
CRAWL4AI_CACHE_MODE=enabled

# 3. å‡å°‘é¡¹ç›®æ•°é‡
max_items=20
```

**Q: å†…å­˜ä¸è¶³**
```python
# 1. é™ä½å¹¶å‘æ•°
CRAWL4AI_MAX_CONCURRENT=2

# 2. åˆ†æ‰¹å¤„ç†
# ä¸è¦ä¸€æ¬¡çˆ¬å–å¤ªå¤šç½‘ç«™

# 3. ä½¿ç”¨Docker
# Dockerèƒ½æ›´å¥½åœ°ç®¡ç†å†…å­˜
```

## è¿ç§»æŒ‡å—

### ğŸ“‹ ä»æ—§ç‰ˆæœ¬è¿ç§»

**1. å¤‡ä»½ç°æœ‰é…ç½®**
```bash
cp config.py config.py.backup
cp requirements.txt requirements.txt.backup
```

**2. å®‰è£…æ–°ä¾èµ–**
```bash
pip install -r requirements.txt
python install_crawl4ai.py
```

**3. æ›´æ–°é…ç½®**
```bash
# å¤åˆ¶æ–°çš„ç¯å¢ƒå˜é‡é…ç½®
cp env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®æ‚¨çš„API keys
```

**4. æµ‹è¯•æ–°åŠŸèƒ½**
```bash
# è¿è¡ŒåŸºæœ¬æµ‹è¯•
python example_crawl4ai_usage.py

# è¿è¡Œå®Œæ•´æµ‹è¯•
python src/crawl4ai_scraper.py
```

### ğŸ”„ é€æ­¥è¿ç§»

**é˜¶æ®µ1: å¹¶è¡Œè¿è¡Œ**
```python
# åŒæ—¶è¿è¡Œæ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬ï¼Œæ¯”è¾ƒç»“æœ
from src.multi_site_scraper import MultiSiteScraper  # æ—§ç‰ˆæœ¬
from src.crawl4ai_scraper import Crawl4AIScraper      # æ–°ç‰ˆæœ¬

# æ¯”è¾ƒç»“æœè´¨é‡å’Œæ€§èƒ½
```

**é˜¶æ®µ2: é…ç½®è°ƒä¼˜**
```python
# æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é…ç½®
# æ‰¾åˆ°æœ€é€‚åˆæ‚¨éœ€æ±‚çš„å‚æ•°ç»„åˆ
```

**é˜¶æ®µ3: å®Œå…¨è¿ç§»**
```python
# åœç”¨æ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬
# æ›´æ–°main.pyä¸­çš„å¯¼å…¥
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†
- ä½¿ç”¨ç¯å¢ƒå˜é‡è€Œéç¡¬ç¼–ç 
- ä¸ºä¸åŒç¯å¢ƒåˆ›å»ºä¸åŒçš„é…ç½®æ–‡ä»¶
- å®šæœŸå¤‡ä»½é…ç½®

### 2. æ€§èƒ½ç›‘æ§
- ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- è®°å½•çˆ¬å–é€Ÿåº¦å’ŒæˆåŠŸç‡
- è®¾ç½®åˆç†çš„è¶…æ—¶å’Œé‡è¯•

### 3. é”™è¯¯å¤„ç†
- å®ç°è¯¦ç»†çš„æ—¥å¿—è®°å½•
- è®¾ç½®é€‚å½“çš„é‡è¯•æœºåˆ¶
- ç›‘æ§å¤±è´¥ç‡å’Œé”™è¯¯ç±»å‹

### 4. æ•°æ®è´¨é‡
- å®šæœŸéªŒè¯æå–çš„æ•°æ®
- ä½¿ç”¨å¤šç§æå–ç­–ç•¥å¯¹æ¯”
- å®ç°æ•°æ®æ¸…æ´—å’ŒéªŒè¯

## ğŸ¤ æ”¯æŒå’Œè´¡çŒ®

å¦‚æœæ‚¨é‡åˆ°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼š

1. **æŸ¥çœ‹æ–‡æ¡£** - é¦–å…ˆæŸ¥çœ‹è¿™ä¸ªREADMEå’Œé…ç½®è¯´æ˜
2. **æ£€æŸ¥Issues** - æŸ¥çœ‹æ˜¯å¦æœ‰ç›¸ä¼¼çš„é—®é¢˜
3. **æäº¤Issue** - è¯¦ç»†æè¿°é—®é¢˜å’Œå¤ç°æ­¥éª¤
4. **è´¡çŒ®ä»£ç ** - æ¬¢è¿æäº¤Pull Request

## ğŸ“š ç›¸å…³èµ„æº

- [Crawl4AIå®˜æ–¹æ–‡æ¡£](https://docs.crawl4ai.com/)
- [Crawl4AI GitHubä»“åº“](https://github.com/unclecode/crawl4ai)
- [Playwrightæ–‡æ¡£](https://playwright.dev/python/)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)

---

ğŸ‰ **ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼** å¦‚æœè¿™ä¸ªå‡çº§ç‰ˆæœ¬å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™é¡¹ç›®ä¸€ä¸ªâ­ï¸ï¼ 