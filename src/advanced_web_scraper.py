#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import random
import requests
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import config

class AdvancedWebScraper:
    """
    ä½¿ç”¨é«˜çº§æŠ€å·§çš„Webçˆ¬è™«ï¼Œä¸“é—¨é’ˆå¯¹theresanaiforthat.com
    ç»“åˆäº†ååçˆ¬è™«æŠ€æœ¯å’Œå¤šç§å¤‡é€‰ç­–ç•¥
    """
    
    def __init__(self):
        self.config = config.Config()
        self.session = requests.Session()
        self.setup_session()
        
    def setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯çš„é«˜çº§é…ç½®"""
        # ä½¿ç”¨æ›´çœŸå®çš„User-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        self.session.headers.update(headers)
        
    def get_chrome_options(self, headless=True):
        """è·å–Chromeæµè§ˆå™¨çš„é«˜çº§é€‰é¡¹é…ç½®"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument("--headless=new")
            
        # ååçˆ¬è™«æŠ€æœ¯
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--allow-running-insecure-content")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        
        # å®éªŒæ€§é€‰é¡¹
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)
        
        # è®¾ç½®éšæœºUser-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
        
        return chrome_options
    
    def scrape_with_selenium(self, url, max_retries=3):
        """ä½¿ç”¨Seleniumçˆ¬å–ç½‘é¡µ"""
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ ä½¿ç”¨Seleniumçˆ¬å– (å°è¯• {attempt + 1}/{max_retries}): {url}")
                
                # è®¾ç½®Chromeé€‰é¡¹
                chrome_options = self.get_chrome_options(headless=True)
                
                # å°è¯•ä½¿ç”¨webdriver-managerè‡ªåŠ¨ç®¡ç†ChromeDriver
                try:
                    service = Service(ChromeDriverManager().install())
                except Exception as e:
                    print(f"âš ï¸ webdriver-managerå¤±è´¥: {e}")
                    # å›é€€åˆ°æ‰‹åŠ¨è·¯å¾„
                    chromedriver_paths = [
                        os.path.join(os.getcwd(), "chromedriver.exe"),
                        os.path.join(os.getcwd(), "drivers", "chromedriver.exe"),
                        r"C:\Program Files\Google\Chrome\Application\chromedriver.exe"
                    ]
                    
                    chromedriver_path = None
                    for path in chromedriver_paths:
                        if os.path.exists(path):
                            chromedriver_path = path
                            break
                    
                    if not chromedriver_path:
                        raise Exception("æ‰¾ä¸åˆ°ChromeDriver")
                    
                    service = Service(executable_path=chromedriver_path)
                
                # åˆ›å»ºWebDriver
                driver = webdriver.Chrome(service=service, options=chrome_options)
                
                try:
                    # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
                    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                    
                    # è®¿é—®ç½‘é¡µ
                    driver.get(url)
                    print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                    
                    # éšæœºç­‰å¾…
                    time.sleep(random.uniform(3, 7))
                    
                    # æ¨¡æ‹Ÿäººç±»è¡Œä¸º - æ»šåŠ¨é¡µé¢
                    self.simulate_human_behavior(driver)
                    
                    # ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    # è·å–é¡µé¢æºç 
                    html_content = driver.page_source
                    
                    # å°è¯•æå–AIå·¥å…·æ•°æ®
                    tools_data = self.extract_ai_tools_selenium(driver)
                    
                    return {
                        'html': html_content,
                        'tools': tools_data,
                        'method': 'selenium',
                        'success': True
                    }
                    
                finally:
                    driver.quit()
                    
            except Exception as e:
                print(f"âŒ Seleniumçˆ¬å–å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(5, 15)
                    print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print("âŒ Seleniumçˆ¬å–æœ€ç»ˆå¤±è´¥")
                    
        return None
    
    def simulate_human_behavior(self, driver):
        """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        try:
            # éšæœºæ»šåŠ¨
            scroll_actions = [
                "window.scrollTo(0, document.body.scrollHeight/4);",
                "window.scrollTo(0, document.body.scrollHeight/2);",
                "window.scrollTo(0, document.body.scrollHeight/3);",
                "window.scrollTo(0, document.body.scrollHeight);",
                "window.scrollTo(0, 0);"
            ]
            
            for action in random.sample(scroll_actions, k=min(3, len(scroll_actions))):
                driver.execute_script(action)
                time.sleep(random.uniform(1, 3))
                
            # æœ€åæ»šåŠ¨åˆ°åº•éƒ¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶å‡ºé”™: {e}")
    
    def extract_ai_tools_selenium(self, driver):
        """ä½¿ç”¨Seleniumæå–AIå·¥å…·æ•°æ®"""
        tools = []
        
        try:
            # é’ˆå¯¹theresanaiforthat.comçš„å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            selectors = [
                "div.tool-card",
                "div[class*='tool']",
                "div[class*='card']",
                "div[class*='item']",
                "article",
                ".tool-item",
                ".ai-tool",
                "[data-tool]",
                "div.grid > div",
                "div.list-item"
            ]
            
            elements = []
            for selector in selectors:
                try:
                    found_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if found_elements:
                        print(f"âœ… æ‰¾åˆ° {len(found_elements)} ä¸ªå…ƒç´ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                        elements = found_elements
                        break
                except Exception as e:
                    print(f"âš ï¸ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            if not elements:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•å·¥å…·å…ƒç´ ")
                return []
            
            # æå–å·¥å…·ä¿¡æ¯
            for i, element in enumerate(elements[:50]):  # é™åˆ¶æœ€å¤šå¤„ç†50ä¸ª
                try:
                    tool_data = self.extract_tool_info_selenium(element, i + 1)
                    if tool_data:
                        tools.append(tool_data)
                        print(f"âœ… æå–å·¥å…· {i + 1}: {tool_data.get('name', 'Unknown')}")
                except Exception as e:
                    print(f"âš ï¸ æå–å·¥å…· {i + 1} å¤±è´¥: {e}")
                    continue
            
            print(f"ğŸ“Š æ€»å…±æå–äº† {len(tools)} ä¸ªAIå·¥å…·")
            return tools
            
        except Exception as e:
            print(f"âŒ æå–AIå·¥å…·æ•°æ®å¤±è´¥: {e}")
            return []
    
    def extract_tool_info_selenium(self, element, index):
        """ä»å•ä¸ªå…ƒç´ ä¸­æå–å·¥å…·ä¿¡æ¯"""
        try:
            # æå–å·¥å…·åç§°
            name_selectors = [
                "h1", "h2", "h3", "h4", "h5", "h6",
                ".title", ".name", ".tool-name", ".tool-title",
                "[class*='title']", "[class*='name']",
                "a[href*='tool']", "a[href*='app']"
            ]
            
            name = ""
            for selector in name_selectors:
                try:
                    name_element = element.find_element(By.CSS_SELECTOR, selector)
                    if name_element and name_element.text.strip():
                        name = name_element.text.strip()
                        break
                except:
                    continue
            
            # æå–æè¿°
            description_selectors = [
                ".description", ".desc", ".summary", ".info",
                "p", ".content", ".text", "[class*='desc']",
                "[class*='summary']", "[class*='info']"
            ]
            
            description = ""
            for selector in description_selectors:
                try:
                    desc_element = element.find_element(By.CSS_SELECTOR, selector)
                    if desc_element and desc_element.text.strip():
                        description = desc_element.text.strip()
                        break
                except:
                    continue
            
            # æå–é“¾æ¥
            link_selectors = [
                "a[href*='http']", "a[href*='www']", "a[href*='tool']",
                "a[href*='app']", "a", "[href]"
            ]
            
            link = ""
            for selector in link_selectors:
                try:
                    link_element = element.find_element(By.CSS_SELECTOR, selector)
                    href = link_element.get_attribute("href")
                    if href and ("http" in href or "www" in href):
                        link = href
                        break
                except:
                    continue
            
            # æå–åˆ†ç±»/æ ‡ç­¾
            category_selectors = [
                ".category", ".tag", ".type", ".genre",
                "[class*='category']", "[class*='tag']", "[class*='type']",
                ".badge", ".label", "span"
            ]
            
            category = ""
            for selector in category_selectors:
                try:
                    cat_element = element.find_element(By.CSS_SELECTOR, selector)
                    if cat_element and cat_element.text.strip():
                        category = cat_element.text.strip()
                        break
                except:
                    continue
            
            # åªè¿”å›æœ‰æ•ˆçš„å·¥å…·æ•°æ®
            if name and len(name) > 2:
                return {
                    'name': name,
                    'description': description,
                    'link': link,
                    'category': category,
                    'source': 'theresanaiforthat.com',
                    'scraped_at': datetime.now().isoformat(),
                    'index': index
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–å·¥å…·ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def scrape_with_requests(self, url, max_retries=3):
        """ä½¿ç”¨requestsçˆ¬å–ç½‘é¡µçš„é«˜çº§ç‰ˆæœ¬"""
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ ä½¿ç”¨requestsçˆ¬å– (å°è¯• {attempt + 1}/{max_retries}): {url}")
                
                # éšæœºç­‰å¾…
                time.sleep(random.uniform(2, 5))
                
                # å‘é€è¯·æ±‚
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    print("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹")
                    
                    # è§£æHTML
                    soup = BeautifulSoup(response.text, 'html.parser')
                    tools_data = self.extract_ai_tools_requests(soup)
                    
                    return {
                        'html': response.text,
                        'tools': tools_data,
                        'method': 'requests',
                        'success': True
                    }
                    
                else:
                    print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ Requestsçˆ¬å–å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(3, 10)
                    print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    
                    # æ›´æ¢User-Agent
                    user_agents = [
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0"
                    ]
                    self.session.headers.update({'User-Agent': random.choice(user_agents)})
                    
        return None
    
    def extract_ai_tools_requests(self, soup):
        """ä½¿ç”¨BeautifulSoupæå–AIå·¥å…·æ•°æ®"""
        tools = []
        
        try:
            # é’ˆå¯¹theresanaiforthat.comçš„å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            selectors = [
                "div.tool-card",
                "div[class*='tool']",
                "div[class*='card']",
                "div[class*='item']",
                "article",
                ".tool-item",
                ".ai-tool",
                "[data-tool]"
            ]
            
            elements = []
            for selector in selectors:
                try:
                    found_elements = soup.select(selector)
                    if found_elements:
                        print(f"âœ… æ‰¾åˆ° {len(found_elements)} ä¸ªå…ƒç´ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                        elements = found_elements
                        break
                except Exception as e:
                    print(f"âš ï¸ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            if not elements:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•å·¥å…·å…ƒç´ ")
                return []
            
            # æå–å·¥å…·ä¿¡æ¯
            for i, element in enumerate(elements[:50]):  # é™åˆ¶æœ€å¤šå¤„ç†50ä¸ª
                try:
                    tool_data = self.extract_tool_info_requests(element, i + 1)
                    if tool_data:
                        tools.append(tool_data)
                        print(f"âœ… æå–å·¥å…· {i + 1}: {tool_data.get('name', 'Unknown')}")
                except Exception as e:
                    print(f"âš ï¸ æå–å·¥å…· {i + 1} å¤±è´¥: {e}")
                    continue
            
            print(f"ğŸ“Š æ€»å…±æå–äº† {len(tools)} ä¸ªAIå·¥å…·")
            return tools
            
        except Exception as e:
            print(f"âŒ æå–AIå·¥å…·æ•°æ®å¤±è´¥: {e}")
            return []
    
    def extract_tool_info_requests(self, element, index):
        """ä»BeautifulSoupå…ƒç´ ä¸­æå–å·¥å…·ä¿¡æ¯"""
        try:
            # æå–å·¥å…·åç§°
            name_selectors = [
                "h1", "h2", "h3", "h4", "h5", "h6",
                ".title", ".name", ".tool-name", ".tool-title",
                "[class*='title']", "[class*='name']"
            ]
            
            name = ""
            for selector in name_selectors:
                try:
                    name_element = element.select_one(selector)
                    if name_element and name_element.get_text().strip():
                        name = name_element.get_text().strip()
                        break
                except:
                    continue
            
            # æå–æè¿°
            description_selectors = [
                ".description", ".desc", ".summary", ".info",
                "p", ".content", ".text"
            ]
            
            description = ""
            for selector in description_selectors:
                try:
                    desc_element = element.select_one(selector)
                    if desc_element and desc_element.get_text().strip():
                        description = desc_element.get_text().strip()
                        break
                except:
                    continue
            
            # æå–é“¾æ¥
            link = ""
            try:
                link_element = element.select_one("a[href]")
                if link_element:
                    link = link_element.get("href", "")
            except:
                pass
            
            # æå–åˆ†ç±»
            category = ""
            try:
                category_element = element.select_one(".category, .tag, .type, .badge")
                if category_element:
                    category = category_element.get_text().strip()
            except:
                pass
            
            # åªè¿”å›æœ‰æ•ˆçš„å·¥å…·æ•°æ®
            if name and len(name) > 2:
                return {
                    'name': name,
                    'description': description,
                    'link': link,
                    'category': category,
                    'source': 'theresanaiforthat.com',
                    'scraped_at': datetime.now().isoformat(),
                    'index': index
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–å·¥å…·ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def scrape_ai_tools(self, url=None):
        """ä¸»è¦çš„çˆ¬å–æ–¹æ³•ï¼Œç»“åˆå¤šç§ç­–ç•¥"""
        if not url:
            url = self.config.TARGET_URL
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–AIå·¥å…·: {url}")
        
        # ç­–ç•¥1: å…ˆå°è¯•Selenium
        result = self.scrape_with_selenium(url)
        if result and result['success'] and result['tools']:
            print(f"âœ… Seleniumçˆ¬å–æˆåŠŸï¼Œè·å¾— {len(result['tools'])} ä¸ªå·¥å…·")
            return result['tools']
        
        # ç­–ç•¥2: å¦‚æœSeleniumå¤±è´¥ï¼Œå°è¯•requests
        print("ğŸ”„ Seleniumå¤±è´¥ï¼Œå°è¯•requestsæ–¹æ³•...")
        result = self.scrape_with_requests(url)
        if result and result['success'] and result['tools']:
            print(f"âœ… Requestsçˆ¬å–æˆåŠŸï¼Œè·å¾— {len(result['tools'])} ä¸ªå·¥å…·")
            return result['tools']
        
        # ç­–ç•¥3: å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        print("âŒ æ‰€æœ‰çˆ¬å–æ–¹æ³•éƒ½å¤±è´¥")
        return [] 