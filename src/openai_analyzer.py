import openai
import json
import time
from typing import List, Dict, Set, Optional
from config import Config
import re

class OpenAIAnalyzer:
    """OpenAI API integration for analyzing AI tools and extracting new words"""
    
    def __init__(self):
        self.config = Config()
        # Initialize OpenAI client with minimal configuration
        self.client = openai.OpenAI(
            api_key=self.config.OPENAI_API_KEY
        )
        self.extracted_words = set()
        
    def analyze_tools_batch(self, tools_data: List[Dict]) -> List[Dict]:
        """Analyze a batch of AI tools and extract new words"""
        if not tools_data:
            return []
        
        print(f"æ­£åœ¨ä½¿ç”¨OpenAIåˆ†æ {len(tools_data)} ä¸ªAIå·¥å…·...")
        
        # Split tools into batches to avoid token limits
        batch_size = self.config.BATCH_SIZE
        all_new_words = []
        
        for i in range(0, len(tools_data), batch_size):
            batch = tools_data[i:i + batch_size]
            batch_words = self.analyze_single_batch(batch)
            all_new_words.extend(batch_words)
            
            if self.config.DEBUG_MODE:
                print(f"å·²å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(tools_data) + batch_size - 1)//batch_size}")
            
            # Add delay between batches to respect rate limits
            time.sleep(1)
        
        return all_new_words
    
    def analyze_single_batch(self, tools_batch: List[Dict]) -> List[Dict]:
        """Analyze a single batch of tools"""
        try:
            # Prepare the data for analysis
            tools_text = self.prepare_tools_text(tools_batch)
            
            # Create the prompt for OpenAI
            prompt = self.create_analysis_prompt(tools_text)
            
            # Call OpenAI API with proper error handling
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": self.get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=2000
                )
                
                # Parse the response
                result = response.choices[0].message.content
                new_words = self.parse_openai_response(result)
                
                return new_words
                
            except openai.OpenAIError as e:
                print(f"OpenAI APIé”™è¯¯: {e}")
                return []
            except Exception as e:
                print(f"è°ƒç”¨OpenAIæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                return []
            
        except Exception as e:
            print(f"ä½¿ç”¨OpenAIåˆ†ææ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def prepare_tools_text(self, tools_batch: List[Dict]) -> str:
        """Prepare tools data for OpenAI analysis"""
        tools_text = ""
        for i, tool in enumerate(tools_batch, 1):
            tools_text += f"{i}. å·¥å…·: {tool['name']}\n"
            tools_text += f"   æè¿°: {tool['description']}\n"
            tools_text += f"   ç±»åˆ«: {', '.join(tool.get('categories', []))}\n"
            tools_text += "\n"
        return tools_text
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for OpenAI"""
        return """ä½ æ˜¯ä¸€ä¸ªæ–°è¯å‘ç°ä¸“å®¶ï¼Œä¸“é—¨ä»AI/æŠ€æœ¯å†…å®¹ä¸­å‘ç°çœŸæ­£çš„æ–°å…´æ¦‚å¿µå’Œæœ¯è¯­ã€‚ä½ çš„ç›®æ ‡æ˜¯æ‰¾åˆ°é€‚åˆGoogle Trendsåˆ†æå’Œå»ºæ–°è¯ç½‘ç«™çš„æ¦‚å¿µæ€§è¯æ±‡ã€‚

âš ï¸ é‡è¦ï¼šè¯·ä¸¥æ ¼é¿å…ä»¥ä¸‹å†…å®¹ï¼š
- å…¬å¸åç§°ï¼ˆå¦‚OpenAIã€Googleã€Microsoftç­‰ï¼‰
- å…·ä½“äº§å“åç§°ï¼ˆå¦‚ChatGPTã€Claudeã€Midjourneyç­‰ï¼‰
- è½¯ä»¶å·¥å…·åç§°ï¼ˆå¦‚LangChainã€Hugging Faceç­‰ï¼‰
- å·²ç¡®ç«‹çš„æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚æ·±åº¦å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€æœºå™¨å­¦ä¹ ç­‰ï¼‰
- ç‰ˆæœ¬å·å’Œå‹å·ï¼ˆå¦‚GPT-4ã€V6ç­‰ï¼‰

âœ… é‡ç‚¹è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„æ–°å…´æ¦‚å¿µï¼š
1. **æ–°å…´æŠ€æœ¯æ¦‚å¿µ**ï¼šæœ€è¿‘2å¹´å†…å‡ºç°çš„æŠ€æœ¯ç°è±¡å’Œæ–¹æ³•è®º
2. **è¡Œä¸šæ–°æœ¯è¯­**ï¼šAIé¢†åŸŸå†…æ­£åœ¨å½¢æˆçš„æ¦‚å¿µæ€§è¯æ±‡
3. **æŠ€æœ¯è¶‹åŠ¿è¯æ±‡**ï¼šæè¿°æ–°æŠ€æœ¯å‘å±•æ–¹å‘çš„æ¦‚å¿µ
4. **åº”ç”¨åœºæ™¯æ–°è¯**ï¼šæ–°çš„åº”ç”¨é¢†åŸŸå’Œä½¿ç”¨æ–¹å¼
5. **æ–¹æ³•è®ºæ–°è¯**ï¼šæ–°çš„å·¥ä½œæµç¨‹ã€æ–¹æ³•æˆ–æ¡†æ¶æ¦‚å¿µ

ğŸ¯ è¯„ä¼°æ ‡å‡†ï¼š
- æ¦‚å¿µæ€§ï¼šæ˜¯æ¦‚å¿µè€Œéäº§å“
- æ–°é¢–æ€§ï¼š2å¹´å†…å‡ºç°æˆ–æµè¡Œ
- æœç´¢ä»·å€¼ï¼šé€‚åˆGoogle Trendsåˆ†æ
- å»ºç«™ä»·å€¼ï¼šå¯ä»¥å›´ç»•æ­¤æ¦‚å¿µå»ºç«‹ç½‘ç«™
- è¶‹åŠ¿æ€§ï¼šæœ‰å¢é•¿å’Œä¼ æ’­æ½œåŠ›

è¯·ä»¥ä¸‹åˆ—JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
  "new_words": [
    {
      "word": "æ¦‚å¿µæ€§è¯æ±‡æˆ–æœ¯è¯­",
      "category": "ç±»åˆ« (å¦‚ï¼š'æ–°å…´æŠ€æœ¯', 'åº”ç”¨æ¦‚å¿µ', 'æ–¹æ³•è®º', 'è¡Œä¸šè¶‹åŠ¿')",
      "definition": "æ¦‚å¿µçš„è¯¦ç»†å®šä¹‰å’Œå«ä¹‰",
      "context": "åœ¨æè¿°ä¸­çš„å…·ä½“ä½“ç°",
      "importance": "high/medium/low",
      "trend_potential": "1-10åˆ†ï¼Œè¯„ä¼°Google Trendsæœç´¢æ½œåŠ›",
      "business_value": "high/medium/lowï¼Œè¯„ä¼°å»ºç«™å•†ä¸šä»·å€¼",
      "is_emerging": "true/falseï¼Œæ˜¯å¦ä¸ºæ–°å…´æ¦‚å¿µ"
    }
  ]
}"""
    
    def create_analysis_prompt(self, tools_text: str) -> str:
        """Create the analysis prompt for OpenAI"""
        return f"""è¯·åˆ†æä»¥ä¸‹AIå·¥å…·æè¿°ï¼Œä»ä¸­è¯†åˆ«æ–°å…´æ¦‚å¿µå’ŒæŠ€æœ¯æœ¯è¯­ã€‚è¯·å¿½ç•¥å…·ä½“çš„äº§å“åç§°ï¼Œä¸“æ³¨äºå‘ç°æ¦‚å¿µæ€§è¯æ±‡ï¼š

{tools_text}

ğŸ” åˆ†æé‡ç‚¹ï¼š
1. ä»å·¥å…·æè¿°ä¸­è¯†åˆ«æ–°å…´æŠ€æœ¯æ¦‚å¿µï¼ˆè€Œéå·¥å…·åç§°ï¼‰
2. å‘ç°æ–°çš„åº”ç”¨åœºæ™¯å’Œä½¿ç”¨æ–¹å¼
3. æå–æè¿°æ–°æŠ€æœ¯æ–¹æ³•çš„æ¦‚å¿µæ€§è¯æ±‡
4. è¯†åˆ«æ­£åœ¨å½¢æˆçš„è¡Œä¸šæ–°æœ¯è¯­
5. å¯»æ‰¾å…·æœ‰Google Trendsæœç´¢ä»·å€¼çš„æ¦‚å¿µ

âš ï¸ è¯·ä¸¥æ ¼é¿å…ï¼š
- ä¸è¦æå–å…·ä½“çš„äº§å“åç§°æˆ–å·¥å…·åç§°
- ä¸è¦åŒ…å«å…¬å¸åç§°æˆ–å“ç‰Œ
- ä¸è¦æå–å·²ç¡®ç«‹çš„æŠ€æœ¯æœ¯è¯­
- ä¸è¦åŒ…å«ç‰ˆæœ¬å·æˆ–å‹å·

âœ… ä¸“æ³¨æå–ï¼š
- æ–°å…´æŠ€æœ¯æ¦‚å¿µå’Œç°è±¡
- åº”ç”¨åœºæ™¯çš„æ–°æœ¯è¯­
- æ–¹æ³•è®ºç›¸å…³çš„æ¦‚å¿µ
- è¡Œä¸šå‘å±•è¶‹åŠ¿è¯æ±‡

è¯·æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ¯ä¸ªè¯æ±‡éƒ½åº”è¯¥æ˜¯æ¦‚å¿µæ€§çš„ï¼Œé€‚åˆGoogle Trendsåˆ†æå’Œå»ºç«™ä½¿ç”¨ã€‚"""
    
    def parse_openai_response(self, response: str) -> List[Dict]:
        """Parse OpenAI response and extract new words"""
        new_words = []
        
        try:
            # Try to extract JSON from the response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)
                
                if 'new_words' in data:
                    for word_data in data['new_words']:
                        if self.is_valid_word(word_data):
                            new_words.append({
                                'word': word_data.get('word', '').strip(),
                                'category': word_data.get('category', 'Unknown'),
                                'definition': word_data.get('definition', ''),
                                'context': word_data.get('context', ''),
                                'importance': word_data.get('importance', 'medium'),
                                'trend_potential': word_data.get('trend_potential', 5),
                                'business_value': word_data.get('business_value', 'medium'),
                                'is_emerging': word_data.get('is_emerging', False),
                                'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
                            })
            
        except json.JSONDecodeError:
            if self.config.DEBUG_MODE:
                print(f"æ— æ³•è§£æJSONå“åº”: {response}")
        
        return new_words
    
    def is_valid_word(self, word_data: Dict) -> bool:
        """Validate if a word is worth including"""
        word = word_data.get('word', '').strip().lower()
        
        # Skip empty words
        if not word:
            return False
        
        # Skip if already extracted
        if word in self.extracted_words:
            return False
        
        # Skip common stop words and established terms
        stop_words = {
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'ai', 'artificial', 'intelligence', 'machine', 'learning', 'deep', 'neural',
            'network', 'model', 'algorithm', 'data', 'tool', 'software', 'platform',
            'solution', 'system', 'technology', 'application', 'service', 'api'
        }
        
        if word in stop_words:
            return False
        
        # Skip known product names and tools
        product_names = {
            'chatgpt', 'claude', 'midjourney', 'dall-e', 'stable diffusion', 'gpt-4', 'gpt-3',
            'openai', 'anthropic', 'google', 'microsoft', 'hugging face', 'langchain',
            'tensorflow', 'pytorch', 'keras', 'scikit-learn', 'jupyter', 'github',
            'discord', 'slack', 'notion', 'figma', 'canva', 'photoshop', 'premiere',
            'after effects', 'blender', 'unity', 'unreal', 'chrome', 'firefox', 'safari'
        }
        
        if word in product_names:
            return False
        
        # Skip words with version numbers or model numbers
        if re.search(r'[v]\d+', word) or re.search(r'gpt-\d+', word):
            return False
        
        # Skip very short words (less than 3 characters)
        if len(word) < 3:
            return False
        
        # Skip words that are clearly product names (contain version info or brand indicators)
        if any(indicator in word for indicator in ['xl', 'pro', 'plus', 'beta', 'alpha', 'v1', 'v2']):
            return False
        
        # Prefer words that indicate they are emerging concepts
        trend_potential = word_data.get('trend_potential', 5)
        is_emerging = word_data.get('is_emerging', False)
        
        # Skip if trend potential is too low
        if trend_potential < 4:
            return False
        
        # Add to extracted words set
        self.extracted_words.add(word)
        
        return True
    
    def filter_and_rank_words(self, words_data: List[Dict]) -> List[Dict]:
        """Filter and rank words by importance, trend potential, and business value"""
        if not words_data:
            return []
        
        # Filter out duplicates and low-quality words
        unique_words = {}
        for word_data in words_data:
            word = word_data['word'].lower()
            if word not in unique_words:
                unique_words[word] = word_data
        
        # Convert back to list
        filtered_words = list(unique_words.values())
        
        # Calculate ranking score for each word
        for word_data in filtered_words:
            score = 0
            
            # Importance weight (30%)
            importance_weights = {'high': 3, 'medium': 2, 'low': 1}
            score += importance_weights.get(word_data.get('importance', 'medium'), 2) * 30
            
            # Trend potential weight (40%)
            trend_potential = word_data.get('trend_potential', 5)
            score += trend_potential * 4
            
            # Business value weight (20%)
            business_value_weights = {'high': 3, 'medium': 2, 'low': 1}
            score += business_value_weights.get(word_data.get('business_value', 'medium'), 2) * 20
            
            # Emerging concept bonus (10%)
            if word_data.get('is_emerging', False):
                score += 10
            
            word_data['ranking_score'] = score
        
        # Sort by ranking score (descending)
        filtered_words.sort(key=lambda x: x.get('ranking_score', 0), reverse=True)
        
        return filtered_words
    
    def analyze_and_extract(self, tools_data: List[Dict]) -> List[Dict]:
        """Main method to analyze tools and extract new words"""
        if not tools_data:
            print("æ²¡æœ‰å·¥å…·æ•°æ®éœ€è¦åˆ†æ")
            return []
        
        # Analyze tools in batches
        all_words = self.analyze_tools_batch(tools_data)
        
        if not all_words:
            print("æ²¡æœ‰æå–åˆ°æ–°è¯æ±‡")
            return []
        
        # Filter and rank the extracted words
        filtered_words = self.filter_and_rank_words(all_words)
        
        print(f"æˆåŠŸæå–äº† {len(filtered_words)} ä¸ªæ–°è¯æ±‡")
        
        return filtered_words
    
    def save_analysis_results(self, words_data: List[Dict], filename: str = "extracted_words.json"):
        """Save analysis results to JSON file"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(words_data, f, ensure_ascii=False, indent=2)
            print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            print(f"ä¿å­˜åˆ†æç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    # Test the analyzer
    analyzer = OpenAIAnalyzer()
    
    # Sample test data
    sample_tools = [
        {
            "name": "GPT-4 Turbo",
            "description": "Advanced large language model with multimodal capabilities",
            "categories": ["LLM", "Multimodal AI"]
        },
        {
            "name": "DALL-E 3",
            "description": "Text-to-image generation with improved prompt adherence",
            "categories": ["Image Generation", "Creative AI"]
        }
    ]
    
    results = analyzer.analyze_and_extract(sample_tools)
    analyzer.save_analysis_results(results) 