#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import random
import requests
from datetime import datetime
from urllib.parse import urljoin, urlparse
import config

class SuperScraper:
    """
    è¶…çº§çˆ¬è™« - ä½¿ç”¨å¤šç§æŠ€æœ¯ç»•è¿‡åçˆ¬è™«æœºåˆ¶
    """
    
    def __init__(self):
        self.config = config.Config()
        self.session = requests.Session()
        self.setup_session()
        
    def setup_session(self):
        """è®¾ç½®é«˜çº§ä¼šè¯é…ç½®"""
        # è½®æ¢User-Agentæ± 
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
        ]
        
        # éšæœºé€‰æ‹©User-Agent
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Sec-GPC': '1'
        }
        
        self.session.headers.update(headers)
        
        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # self.session.proxies = {'http': 'http://proxy:port', 'https': 'https://proxy:port'}
    
    def get_with_retry(self, url, max_retries=5, backoff_factor=1.5):
        """
        å¸¦é‡è¯•å’Œé€€é¿ç­–ç•¥çš„GETè¯·æ±‚
        """
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ è¯·æ±‚å°è¯• {attempt + 1}/{max_retries}: {url}")
                
                # éšæœºç­‰å¾…
                time.sleep(random.uniform(2, 6))
                
                # è½®æ¢User-Agent
                self.session.headers['User-Agent'] = random.choice(self.user_agents)
                
                # æ·»åŠ æ›´å¤šéšæœºå¤´éƒ¨
                random_headers = {
                    'Accept': random.choice([
                        'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                        'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    ]),
                    'Accept-Language': random.choice([
                        'en-US,en;q=0.9',
                        'en-US,en;q=0.8',
                        'en-US,en;q=0.9,zh-CN;q=0.8',
                        'en-GB,en;q=0.9'
                    ]),
                    'Sec-CH-UA': random.choice([
                        '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                        '"Not_A Brand";v="8", "Chromium";v="119", "Google Chrome";v="119"',
                        '"Not_A Brand";v="8", "Chromium";v="118", "Google Chrome";v="118"'
                    ]),
                    'Sec-CH-UA-Mobile': '?0',
                    'Sec-CH-UA-Platform': random.choice(['"Windows"', '"macOS"', '"Linux"'])
                }
                
                # æ›´æ–°éšæœºå¤´éƒ¨
                self.session.headers.update(random_headers)
                
                # å‘é€è¯·æ±‚
                response = self.session.get(url, timeout=30)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    print(f"âœ… æˆåŠŸè·å–é¡µé¢ (çŠ¶æ€ç : {response.status_code})")
                    return response
                elif response.status_code == 403:
                    print(f"âŒ è¢«æ‹’ç»è®¿é—® (çŠ¶æ€ç : {response.status_code})")
                    # å°è¯•ä¸åŒçš„ç­–ç•¥
                    if attempt < max_retries - 1:
                        print("ğŸ”„ å°è¯•ä¸åŒçš„è¯·æ±‚ç­–ç•¥...")
                        self.try_alternative_request_strategy(url)
                elif response.status_code == 429:
                    print(f"âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ (çŠ¶æ€ç : {response.status_code})")
                    wait_time = backoff_factor ** attempt * 10
                    print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                else:
                    print(f"âš ï¸ æ„å¤–çŠ¶æ€ç : {response.status_code}")
                    
            except requests.exceptions.Timeout:
                print(f"â° è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1})")
            except requests.exceptions.ConnectionError as e:
                print(f"ğŸ”Œ è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
            except Exception as e:
                print(f"âŒ å…¶ä»–é”™è¯¯ (å°è¯• {attempt + 1}): {e}")
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = backoff_factor ** attempt * random.uniform(3, 8)
                print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        print(f"âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†")
        return None
    
    def try_alternative_request_strategy(self, url):
        """å°è¯•å¤‡é€‰è¯·æ±‚ç­–ç•¥"""
        strategies = [
            self.strategy_google_cache,
            self.strategy_wayback_machine,
            self.strategy_mobile_user_agent,
            self.strategy_different_referer
        ]
        
        for strategy in strategies:
            try:
                result = strategy(url)
                if result:
                    return result
            except Exception as e:
                print(f"âš ï¸ å¤‡é€‰ç­–ç•¥å¤±è´¥: {e}")
                continue
        
        return None
    
    def strategy_google_cache(self, url):
        """å°è¯•é€šè¿‡Googleç¼“å­˜è®¿é—®"""
        print("ğŸ”„ å°è¯•Googleç¼“å­˜...")
        cache_url = f"https://webcache.googleusercontent.com/search?q=cache:{url}"
        try:
            response = self.session.get(cache_url, timeout=30)
            if response.status_code == 200:
                print("âœ… Googleç¼“å­˜æˆåŠŸ")
                return response
        except:
            pass
        return None
    
    def strategy_wayback_machine(self, url):
        """å°è¯•é€šè¿‡Wayback Machineè®¿é—®"""
        print("ğŸ”„ å°è¯•Wayback Machine...")
        wayback_url = f"https://web.archive.org/web/{url}"
        try:
            response = self.session.get(wayback_url, timeout=30)
            if response.status_code == 200:
                print("âœ… Wayback MachineæˆåŠŸ")
                return response
        except:
            pass
        return None
    
    def strategy_mobile_user_agent(self, url):
        """å°è¯•ç§»åŠ¨ç«¯User-Agent"""
        print("ğŸ”„ å°è¯•ç§»åŠ¨ç«¯User-Agent...")
        mobile_agents = [
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (Android 14; Mobile; rv:109.0) Gecko/109.0 Firefox/121.0",
            "Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
        ]
        
        original_ua = self.session.headers.get('User-Agent')
        try:
            self.session.headers['User-Agent'] = random.choice(mobile_agents)
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                print("âœ… ç§»åŠ¨ç«¯User-AgentæˆåŠŸ")
                return response
        except:
            pass
        finally:
            self.session.headers['User-Agent'] = original_ua
        return None
    
    def strategy_different_referer(self, url):
        """å°è¯•ä¸åŒçš„Referer"""
        print("ğŸ”„ å°è¯•ä¸åŒçš„Referer...")
        referers = [
            "https://www.google.com/",
            "https://www.bing.com/",
            "https://duckduckgo.com/",
            "https://www.reddit.com/",
            "https://twitter.com/",
            "https://www.facebook.com/"
        ]
        
        original_referer = self.session.headers.get('Referer')
        try:
            self.session.headers['Referer'] = random.choice(referers)
            response = self.session.get(url, timeout=30)
            if response.status_code == 200:
                print("âœ… ä¸åŒRefereræˆåŠŸ")
                return response
        except:
            pass
        finally:
            if original_referer:
                self.session.headers['Referer'] = original_referer
            else:
                self.session.headers.pop('Referer', None)
        return None
    
    def extract_ai_tools_from_html(self, html_content, url):
        """ä»HTMLå†…å®¹ä¸­æå–AIå·¥å…·"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            tools = []
            
            # é’ˆå¯¹theresanaiforthat.comçš„ç‰¹æ®Šé€‰æ‹©å™¨
            selectors = [
                # å¸¸è§çš„å·¥å…·å¡ç‰‡é€‰æ‹©å™¨
                "div.tool-card",
                "div.tool-item",
                "div.ai-tool",
                "div[class*='tool']",
                "div[class*='card']",
                "div[class*='item']",
                "article",
                ".tool",
                ".card",
                ".item",
                "[data-tool]",
                "[data-item]",
                
                # æ›´é€šç”¨çš„é€‰æ‹©å™¨
                "div.grid > div",
                "div.list > div",
                "div.container > div",
                "main > div",
                "section > div",
                
                # é“¾æ¥ç›¸å…³çš„é€‰æ‹©å™¨
                "a[href*='tool']",
                "a[href*='app']",
                "a[href*='ai']",
                
                # åŸºäºæ–‡æœ¬å†…å®¹çš„é€‰æ‹©å™¨
                "div:contains('AI')",
                "div:contains('tool')",
                "div:contains('app')"
            ]
            
            print(f"ğŸ” å°è¯•æå–AIå·¥å…·æ•°æ®...")
            
            for selector in selectors:
                try:
                    # è·³è¿‡åŒ…å«æ–‡æœ¬çš„é€‰æ‹©å™¨ï¼ŒBeautifulSoupä¸ç›´æ¥æ”¯æŒ
                    if ':contains(' in selector:
                        continue
                        
                    elements = soup.select(selector)
                    if elements:
                        print(f"âœ… æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                        
                        for i, element in enumerate(elements[:30]):  # é™åˆ¶æœ€å¤š30ä¸ª
                            try:
                                tool_data = self.extract_tool_from_element(element, i + 1, url)
                                if tool_data:
                                    tools.append(tool_data)
                            except Exception as e:
                                print(f"âš ï¸ æå–å·¥å…· {i+1} å¤±è´¥: {e}")
                                continue
                        
                        # å¦‚æœæ‰¾åˆ°äº†å·¥å…·ï¼Œå°±ä¸å†å°è¯•å…¶ä»–é€‰æ‹©å™¨
                        if tools:
                            break
                            
                except Exception as e:
                    print(f"âš ï¸ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ï¼Œå°è¯•æ›´é€šç”¨çš„æ–¹æ³•
            if not tools:
                print("ğŸ”„ å°è¯•é€šç”¨æ–‡æœ¬æå–...")
                tools = self.extract_tools_generic(soup, url)
            
            print(f"ğŸ“Š æ€»å…±æå–äº† {len(tools)} ä¸ªAIå·¥å…·")
            return tools
            
        except Exception as e:
            print(f"âŒ æå–AIå·¥å…·å¤±è´¥: {e}")
            return []
    
    def extract_tool_from_element(self, element, index, base_url):
        """ä»å•ä¸ªå…ƒç´ æå–å·¥å…·ä¿¡æ¯"""
        try:
            # æå–åç§°
            name = ""
            name_selectors = [
                "h1", "h2", "h3", "h4", "h5", "h6",
                ".title", ".name", ".tool-name", ".tool-title",
                "[class*='title']", "[class*='name']",
                "a", "strong", "b"
            ]
            
            for selector in name_selectors:
                try:
                    name_elem = element.select_one(selector)
                    if name_elem and name_elem.get_text().strip():
                        name = name_elem.get_text().strip()
                        break
                except:
                    continue
            
            # æå–æè¿°
            description = ""
            desc_selectors = [
                ".description", ".desc", ".summary", ".info",
                "p", ".content", ".text", ".details",
                "[class*='desc']", "[class*='summary']"
            ]
            
            for selector in desc_selectors:
                try:
                    desc_elem = element.select_one(selector)
                    if desc_elem and desc_elem.get_text().strip():
                        description = desc_elem.get_text().strip()
                        break
                except:
                    continue
            
            # æå–é“¾æ¥
            link = ""
            try:
                link_elem = element.select_one("a[href]")
                if link_elem:
                    href = link_elem.get('href', '')
                    if href:
                        # è½¬æ¢ä¸ºç»å¯¹URL
                        link = urljoin(base_url, href)
            except:
                pass
            
            # æå–åˆ†ç±»
            category = ""
            cat_selectors = [
                ".category", ".tag", ".type", ".genre",
                ".badge", ".label", "span"
            ]
            
            for selector in cat_selectors:
                try:
                    cat_elem = element.select_one(selector)
                    if cat_elem and cat_elem.get_text().strip():
                        category = cat_elem.get_text().strip()
                        break
                except:
                    continue
            
            # éªŒè¯æ•°æ®è´¨é‡
            if name and len(name.strip()) > 2:
                return {
                    'name': name,
                    'description': description,
                    'link': link,
                    'category': category,
                    'source': 'theresanaiforthat.com',
                    'scraped_at': datetime.now().isoformat(),
                    'index': index
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–å…ƒç´ ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def extract_tools_generic(self, soup, base_url):
        """é€šç”¨çš„å·¥å…·æå–æ–¹æ³•"""
        tools = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«å·¥å…·ä¿¡æ¯çš„å…ƒç´ 
            all_links = soup.find_all('a', href=True)
            
            for i, link in enumerate(all_links[:100]):  # é™åˆ¶æ£€æŸ¥å‰100ä¸ªé“¾æ¥
                try:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    # è¿‡æ»¤å¯èƒ½çš„å·¥å…·é“¾æ¥
                    if (text and len(text) > 3 and 
                        any(keyword in text.lower() for keyword in ['ai', 'tool', 'app', 'platform', 'software']) and
                        not any(skip in href.lower() for skip in ['#', 'javascript:', 'mailto:', 'tel:'])):
                        
                        # è·å–çˆ¶å…ƒç´ çš„æ›´å¤šä¿¡æ¯
                        parent = link.parent
                        description = ""
                        
                        # å°è¯•è·å–æè¿°
                        if parent:
                            desc_text = parent.get_text().strip()
                            if desc_text and len(desc_text) > len(text):
                                description = desc_text
                        
                        tool_data = {
                            'name': text,
                            'description': description,
                            'link': urljoin(base_url, href),
                            'category': 'Unknown',
                            'source': 'theresanaiforthat.com',
                            'scraped_at': datetime.now().isoformat(),
                            'index': i + 1
                        }
                        
                        tools.append(tool_data)
                        
                except Exception as e:
                    continue
            
            return tools
            
        except Exception as e:
            print(f"âŒ é€šç”¨æå–å¤±è´¥: {e}")
            return []
    
    def scrape_ai_tools(self, url=None):
        """ä¸»è¦çš„çˆ¬å–æ–¹æ³•"""
        if not url:
            url = self.config.TARGET_URL
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–AIå·¥å…·: {url}")
        
        # è·å–ç½‘é¡µå†…å®¹
        response = self.get_with_retry(url)
        
        if response and response.status_code == 200:
            print("âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹")
            
            # æå–AIå·¥å…·æ•°æ®
            tools = self.extract_ai_tools_from_html(response.text, url)
            
            if tools:
                print(f"ğŸ‰ æˆåŠŸæå– {len(tools)} ä¸ªAIå·¥å…·")
                return tools
            else:
                print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•å·¥å…·æ•°æ®")
                return []
        else:
            print("âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹")
            return [] 