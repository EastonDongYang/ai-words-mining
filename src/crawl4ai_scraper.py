#!/usr/bin/env python3
"""
Crawl4AI Enhanced Scraper for AI Words Mining System
åŸºäºcrawl4aiçš„é«˜æ€§èƒ½AIå·¥å…·çˆ¬è™«
"""

import asyncio
import json
import time
from typing import List, Dict, Optional, Any
from datetime import datetime
from urllib.parse import urljoin, urlparse
from pydantic import BaseModel, Field

# Crawl4AI imports - ä¿®æ­£å¯¼å…¥è¯­å¥
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
    from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
except ImportError as e:
    print(f"âŒ Crawl4AIå¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…crawl4ai: pip install crawl4ai")
    raise

from config import Config


class AIToolExtractor:
    """AIå·¥å…·ä¿¡æ¯æå–å™¨"""
    
    def __init__(self):
        self.config = Config()
        
    def get_toolify_schema(self) -> dict:
        """Toolify.aiçš„æå–schema"""
        return {
            "name": "Toolify AI Tools",
            "baseSelector": ".tool-card, .tool-item, .grid-item",
            "fields": [
                {
                    "name": "tool_name",
                    "selector": "h3, h4, .tool-name, .title, a[href*='/tool/']",
                    "type": "text",
                },
                {
                    "name": "description",
                    "selector": ".description, .tool-description, p",
                    "type": "text",
                },
                {
                    "name": "link",
                    "selector": "a[href*='/tool/'], a[href*='/ai/']",
                    "type": "attribute",
                    "attribute": "href"
                },
                {
                    "name": "categories",
                    "selector": ".tag, .category, .badge, .label",
                    "type": "list",
                    "fields": [
                        {"name": "category", "selector": "", "type": "text"}
                    ]
                }
            ]
        }
    
    def get_producthunt_schema(self) -> dict:
        """Product Huntçš„æå–schema"""
        return {
            "name": "Product Hunt Tools",
            "baseSelector": "[data-test*='post'], .item, .product-item",
            "fields": [
                {
                    "name": "tool_name",
                    "selector": "h3, h4, .name, .title, a[data-test*='post-name']",
                    "type": "text",
                },
                {
                    "name": "description",
                    "selector": ".description, .tagline, p",
                    "type": "text",
                },
                {
                    "name": "link",
                    "selector": "a[href*='/posts/']",
                    "type": "attribute",
                    "attribute": "href"
                },
                {
                    "name": "upvotes",
                    "selector": ".vote-count, .upvote",
                    "type": "text",
                }
            ]
        }
    
    def get_futuretools_schema(self) -> dict:
        """Future Toolsçš„æå–schema"""
        return {
            "name": "Future Tools",
            "baseSelector": ".tool-card, .tool-item, .grid-item",
            "fields": [
                {
                    "name": "tool_name",
                    "selector": "h3, h4, .tool-name, .title",
                    "type": "text",
                },
                {
                    "name": "description",
                    "selector": ".description, .tool-description, p",
                    "type": "text",
                },
                {
                    "name": "link",
                    "selector": "a",
                    "type": "attribute",
                    "attribute": "href"
                },
                {
                    "name": "pricing",
                    "selector": ".price, .pricing, .cost",
                    "type": "text",
                }
            ]
        }


class AIToolModel(BaseModel):
    """AIå·¥å…·æ•°æ®æ¨¡å‹ï¼ˆç”¨äºLLMæå–ï¼‰"""
    tool_name: str = Field(..., description="AIå·¥å…·çš„åç§°")
    description: str = Field(..., description="å·¥å…·çš„è¯¦ç»†æè¿°")
    categories: List[str] = Field(default_factory=list, description="å·¥å…·çš„åˆ†ç±»æ ‡ç­¾")
    pricing: Optional[str] = Field(None, description="å®šä»·ä¿¡æ¯")
    features: List[str] = Field(default_factory=list, description="ä¸»è¦åŠŸèƒ½ç‰¹ç‚¹")
    link: Optional[str] = Field(None, description="å·¥å…·é“¾æ¥")


class Crawl4AIScraper:
    """åŸºäºCrawl4AIçš„é«˜æ€§èƒ½çˆ¬è™«"""
    
    def __init__(self, debug_mode: bool = False):
        self.config = Config()
        self.debug_mode = debug_mode
        self.extractor = AIToolExtractor()
        
        # æµè§ˆå™¨é…ç½®
        self.browser_config = BrowserConfig(
            headless=True,
            verbose=debug_mode,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            java_script_enabled=True,
            # åçˆ¬é…ç½®
            viewport={"width": 1920, "height": 1080},
            locale="en-US",
            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡é¿å…é‡å¤åˆå§‹åŒ–
            user_data_dir="./browser_profile",
            use_persistent_context=True,
        )
    
    async def scrape_site_with_css(self, url: str, site_name: str, max_items: int = 50) -> List[Dict]:
        """ä½¿ç”¨CSSé€‰æ‹©å™¨ç­–ç•¥çˆ¬å–ç½‘ç«™"""
        
        # æ ¹æ®ç½‘ç«™é€‰æ‹©schema
        schema_map = {
            "toolify": self.extractor.get_toolify_schema(),
            "producthunt": self.extractor.get_producthunt_schema(),
            "futuretools": self.extractor.get_futuretools_schema(),
        }
        
        schema = schema_map.get(site_name, self.extractor.get_toolify_schema())
        
        # åˆ›å»ºæå–ç­–ç•¥
        extraction_strategy = JsonCssExtractionStrategy(
            schema=schema,
            verbose=self.debug_mode
        )
        
        # åˆ›å»ºMarkdownç”Ÿæˆå™¨ï¼ˆå¸¦å†…å®¹è¿‡æ»¤ï¼‰
        markdown_generator = DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(
                threshold=0.48,
                threshold_type="fixed",
                min_word_threshold=10
            )
        )
        
        # çˆ¬å–é…ç½®
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            extraction_strategy=extraction_strategy,
            markdown_generator=markdown_generator,
            output_formats=['extracted_content', 'markdown'],
            # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½
            wait_for_timeout=3000,
            # æ‰§è¡Œæ»šåŠ¨ä»¥åŠ è½½æ›´å¤šå†…å®¹
            js_code=[
                "window.scrollTo(0, document.body.scrollHeight);",
                "await new Promise(resolve => setTimeout(resolve, 2000));",
                "window.scrollTo(0, document.body.scrollHeight);"
            ],
            # æˆªå›¾ç”¨äºè°ƒè¯•
            screenshot=self.debug_mode,
        )
        
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            if self.debug_mode:
                print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–: {site_name} - {url}")
            
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success and result.extracted_content:
                try:
                    extracted_data = json.loads(result.extracted_content)
                    
                    # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
                    cleaned_data = self.clean_and_standardize_data(
                        extracted_data, site_name, max_items
                    )
                    
                    if self.debug_mode:
                        print(f"âœ… æˆåŠŸä» {site_name} æå– {len(cleaned_data)} ä¸ªå·¥å…·")
                    
                    return cleaned_data
                
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æé”™è¯¯ ({site_name}): {e}")
                    return []
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥ ({site_name}): {result.error_message}")
                return []
    
    async def scrape_site_with_llm(self, url: str, site_name: str, max_items: int = 50) -> List[Dict]:
        """ä½¿ç”¨LLMç­–ç•¥çˆ¬å–ç½‘ç«™ï¼ˆéœ€è¦API keyï¼‰"""
        
        if not self.config.OPENAI_API_KEY:
            print("âš ï¸ æœªé…ç½®OpenAI API Keyï¼Œè·³è¿‡LLMæå–")
            return []
        
        # åˆ›å»ºLLMæå–ç­–ç•¥
        llm_strategy = LLMExtractionStrategy(
            provider="openai/gpt-4o-mini",  # ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
            api_token=self.config.OPENAI_API_KEY,
            schema=AIToolModel.schema(),
            extraction_type="schema",
            instruction=f"""
            ä»ç½‘é¡µå†…å®¹ä¸­æå–AIå·¥å…·ä¿¡æ¯ã€‚æ¯ä¸ªå·¥å…·åº”åŒ…å«ï¼š
            1. å·¥å…·åç§°ï¼ˆtool_nameï¼‰
            2. è¯¦ç»†æè¿°ï¼ˆdescriptionï¼‰
            3. åˆ†ç±»æ ‡ç­¾ï¼ˆcategoriesï¼‰
            4. å®šä»·ä¿¡æ¯ï¼ˆpricingï¼Œå¦‚æœæœ‰ï¼‰
            5. ä¸»è¦åŠŸèƒ½ç‰¹ç‚¹ï¼ˆfeaturesï¼‰
            6. å·¥å…·é“¾æ¥ï¼ˆlinkï¼‰
            
            è¯·ç¡®ä¿æå–çš„å·¥å…·åç§°æ¸…æ™°ã€æè¿°å‡†ç¡®ã€‚å¿½ç•¥å¯¼èˆªé“¾æ¥ã€å¹¿å‘Šç­‰æ— å…³å†…å®¹ã€‚
            æœ€å¤šæå– {max_items} ä¸ªå·¥å…·ã€‚
            """,
            # ä½¿ç”¨BM25è¿‡æ»¤ç›¸å…³å†…å®¹
            content_filter=BM25ContentFilter(
                user_query="AI tools artificial intelligence machine learning",
                bm25_threshold=1.0
            )
        )
        
        # çˆ¬å–é…ç½®
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            extraction_strategy=llm_strategy,
            output_formats=['extracted_content', 'markdown'],
            wait_for_timeout=3000,
            js_code=[
                "window.scrollTo(0, document.body.scrollHeight);",
                "await new Promise(resolve => setTimeout(resolve, 2000));"
            ],
        )
        
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            if self.debug_mode:
                print(f"ğŸ¤– ä½¿ç”¨LLMçˆ¬å–: {site_name} - {url}")
            
            result = await crawler.arun(url=url, config=crawl_config)
            
            if result.success and result.extracted_content:
                try:
                    extracted_data = json.loads(result.extracted_content)
                    
                    # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    if isinstance(extracted_data, dict):
                        extracted_data = [extracted_data]
                    
                    # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
                    cleaned_data = self.clean_and_standardize_data(
                        extracted_data, site_name, max_items
                    )
                    
                    if self.debug_mode:
                        print(f"âœ… LLMæˆåŠŸä» {site_name} æå– {len(cleaned_data)} ä¸ªå·¥å…·")
                    
                    return cleaned_data
                
                except json.JSONDecodeError as e:
                    print(f"âŒ LLM JSONè§£æé”™è¯¯ ({site_name}): {e}")
                    return []
            else:
                print(f"âŒ LLMçˆ¬å–å¤±è´¥ ({site_name}): {result.error_message}")
                return []
    
    def clean_and_standardize_data(self, data: List[Dict], source: str, max_items: int) -> List[Dict]:
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®"""
        cleaned_data = []
        
        for item in data[:max_items]:
            try:
                # æå–å’Œæ¸…æ´—åç§°
                name = self.extract_text_field(item, ['tool_name', 'name', 'title'])
                if not name or len(name.strip()) < 2:
                    continue
                
                # æå–å’Œæ¸…æ´—æè¿°
                description = self.extract_text_field(item, ['description', 'tagline', 'summary'])
                if not description:
                    description = "AIå·¥å…·æè¿°æš‚æ— "
                
                # æå–åˆ†ç±»
                categories = self.extract_categories(item)
                
                # æå–é“¾æ¥
                link = self.extract_text_field(item, ['link', 'url', 'href'])
                if link and not link.startswith('http'):
                    # æ„å»ºå®Œæ•´URL
                    if source == 'toolify':
                        link = f"https://www.toolify.ai{link}"
                    elif source == 'producthunt':
                        link = f"https://www.producthunt.com{link}"
                    elif source == 'futuretools':
                        link = f"https://www.futuretools.io{link}"
                
                # åˆ›å»ºæ ‡å‡†åŒ–æ•°æ®
                standardized_item = {
                    'name': name.strip(),
                    'description': description.strip(),
                    'categories': categories,
                    'link': link,
                    'source': source,
                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'pricing': self.extract_text_field(item, ['pricing', 'price', 'cost']),
                    'features': self.extract_list_field(item, ['features', 'capabilities']),
                    'upvotes': self.extract_text_field(item, ['upvotes', 'votes'])
                }
                
                cleaned_data.append(standardized_item)
                
            except Exception as e:
                if self.debug_mode:
                    print(f"âš ï¸ æ¸…æ´—æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        return cleaned_data
    
    def extract_text_field(self, item: Dict, field_names: List[str]) -> Optional[str]:
        """æå–æ–‡æœ¬å­—æ®µ"""
        for field_name in field_names:
            if field_name in item:
                value = item[field_name]
                if isinstance(value, str) and value.strip():
                    return value.strip()
                elif isinstance(value, list) and value:
                    return str(value[0]).strip()
        return None
    
    def extract_list_field(self, item: Dict, field_names: List[str]) -> List[str]:
        """æå–åˆ—è¡¨å­—æ®µ"""
        for field_name in field_names:
            if field_name in item:
                value = item[field_name]
                if isinstance(value, list):
                    return [str(v).strip() for v in value if str(v).strip()]
                elif isinstance(value, str) and value.strip():
                    return [value.strip()]
        return []
    
    def extract_categories(self, item: Dict) -> List[str]:
        """æå–åˆ†ç±»ä¿¡æ¯"""
        categories = []
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„åˆ†ç±»å­—æ®µ
        for field_name in ['categories', 'tags', 'category', 'tag']:
            if field_name in item:
                value = item[field_name]
                if isinstance(value, list):
                    for cat in value:
                        if isinstance(cat, dict) and 'category' in cat:
                            categories.append(cat['category'])
                        elif isinstance(cat, str) and cat.strip():
                            categories.append(cat.strip())
                elif isinstance(value, str) and value.strip():
                    categories.append(value.strip())
        
        return categories[:5]  # é™åˆ¶åˆ†ç±»æ•°é‡
    
    async def scrape_multiple_sites(self, urls_config: Dict[str, Dict], use_llm: bool = False) -> List[Dict]:
        """å¹¶å‘çˆ¬å–å¤šä¸ªç«™ç‚¹"""
        all_tools = []
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for url, config in urls_config.items():
            site_name = config.get('name', 'unknown')
            max_items = config.get('max_items', 50)
            
            if use_llm:
                task = self.scrape_site_with_llm(url, site_name, max_items)
            else:
                task = self.scrape_site_with_css(url, site_name, max_items)
            
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ ä»»åŠ¡ {i} æ‰§è¡Œå¤±è´¥: {result}")
            else:
                all_tools.extend(result)
        
        # å»é‡
        unique_tools = self.remove_duplicates(all_tools)
        
        print(f"ğŸ“Š æ€»å…±çˆ¬å–åˆ° {len(unique_tools)} ä¸ªç‹¬ç‰¹çš„AIå·¥å…·")
        return unique_tools
    
    def remove_duplicates(self, tools: List[Dict]) -> List[Dict]:
        """æ ¹æ®å·¥å…·åç§°å»é‡"""
        seen_names = set()
        unique_tools = []
        
        for tool in tools:
            name = tool.get('name', '').lower().strip()
            if name and name not in seen_names:
                seen_names.add(name)
                unique_tools.append(tool)
        
        return unique_tools
    
    async def deep_crawl_site(self, start_url: str, max_pages: int = 10) -> List[Dict]:
        """æ·±åº¦çˆ¬å–ç½‘ç«™ï¼ˆä½¿ç”¨BFSç­–ç•¥ï¼‰"""
        
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            output_formats=['markdown', 'links'],
            # åªçˆ¬å–AIå·¥å…·ç›¸å…³é¡µé¢
            include_patterns=[
                r".*/(tool|ai|product|app)/.*",
                r".*/tools/.*",
                r".*/ai-tools/.*"
            ],
            exclude_patterns=[
                r".*/(login|register|contact|about|privacy|terms)/.*",
                r".*/\.(jpg|jpeg|png|gif|pdf|doc|docx)$"
            ],
            scope='domain',
            delay_between_requests=1.0,
            respect_robots_txt=True,
            max_retries=3,
            wait_for_timeout=2000,
        )
        
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            print(f"ğŸ”„ å¼€å§‹æ·±åº¦çˆ¬å–: {start_url}")
            
            crawl_generator = await crawler.adeep_crawl(
                start_url=start_url,
                strategy="bfs",  # å¹¿åº¦ä¼˜å…ˆæœç´¢
                max_depth=3,
                max_pages=max_pages,
                config=crawl_config
            )
            
            all_tools = []
            page_count = 0
            
            async for result in crawl_generator:
                page_count += 1
                
                if result.success:
                    print(f"[{page_count:02d}] æ·±åº¦: {result.depth}, URL: {result.url}")
                    
                    # ä»markdownå†…å®¹ä¸­æå–å·¥å…·ä¿¡æ¯
                    # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æå–é€»è¾‘
                    if result.markdown and 'AI' in result.markdown.fit_markdown:
                        # ç®€å•çš„å·¥å…·ä¿¡æ¯æå–
                        tool_info = {
                            'name': f"å‘ç°çš„å·¥å…·_{page_count}",
                            'description': result.markdown.fit_markdown[:200] + "...",
                            'categories': ['AI', 'Tools'],
                            'link': result.url,
                            'source': 'deep_crawl',
                            'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'depth': result.depth
                        }
                        all_tools.append(tool_info)
                
                else:
                    print(f"[{page_count:02d}] å¤±è´¥: {result.url}, é”™è¯¯: {result.error_message}")
            
            print(f"âœ… æ·±åº¦çˆ¬å–å®Œæˆï¼Œè®¿é—®äº† {page_count} ä¸ªé¡µé¢ï¼Œå‘ç° {len(all_tools)} ä¸ªå·¥å…·")
            return all_tools


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    scraper = Crawl4AIScraper(debug_mode=True)
    
    # é…ç½®è¦çˆ¬å–çš„ç«™ç‚¹
    urls_config = {
        "https://www.toolify.ai/": {
            "name": "toolify",
            "max_items": 30
        },
        "https://www.producthunt.com/topics/artificial-intelligence": {
            "name": "producthunt",
            "max_items": 25
        },
        "https://www.futuretools.io/": {
            "name": "futuretools",
            "max_items": 20
        }
    }
    
    try:
        # 1. ä½¿ç”¨CSSé€‰æ‹©å™¨ç­–ç•¥çˆ¬å–
        print("ğŸš€ å¼€å§‹ä½¿ç”¨CSSé€‰æ‹©å™¨ç­–ç•¥çˆ¬å–...")
        css_results = await scraper.scrape_multiple_sites(urls_config, use_llm=False)
        
        # 2. ä½¿ç”¨LLMç­–ç•¥çˆ¬å–ï¼ˆå¯é€‰ï¼‰
        print("\nğŸ¤– å¼€å§‹ä½¿ç”¨LLMç­–ç•¥çˆ¬å–...")
        llm_results = await scraper.scrape_multiple_sites(urls_config, use_llm=True)
        
        # 3. æ·±åº¦çˆ¬å–ç¤ºä¾‹
        print("\nğŸ”„ å¼€å§‹æ·±åº¦çˆ¬å–...")
        deep_results = await scraper.deep_crawl_site("https://www.toolify.ai/", max_pages=5)
        
        # åˆå¹¶ç»“æœ
        all_results = css_results + llm_results + deep_results
        unique_results = scraper.remove_duplicates(all_results)
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ: {len(unique_results)} ä¸ªç‹¬ç‰¹çš„AIå·¥å…·")
        
        # ä¿å­˜ç»“æœ
        with open('crawl4ai_results.json', 'w', encoding='utf-8') as f:
            json.dump(unique_results, f, ensure_ascii=False, indent=2)
        
        print("âœ… ç»“æœå·²ä¿å­˜åˆ° crawl4ai_results.json")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 